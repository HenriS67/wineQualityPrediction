{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: LES ARBRES DE DECISION: \n",
    "Nous allons tester maintenant les arbres de décisions. Dans ces arbres chaque noeud divise les observations en 2 branches en fonction d'une condition (par exemple alcohol<=12) et chaque feuille de l'arbre correspondra à une qualité égale à la moyenne de la qualité des observations présentes dans la feuille.\n",
    "Une nouvelle observation x n'aura qu'à suivre le chemin de l'arbre en fonction des conditions des noeuds pour atterir dans une feuille. La prédiction sera alors la qualité de la feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 10)\n",
      "(1074,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hists(df):\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    for n in range(12):\n",
    "        i = n % 3\n",
    "        j = n % 4\n",
    "        ax[i, j].hist(df.iloc[:, n], bins='auto')\n",
    "        ax[i, j].set_xlabel(df.columns[n])\n",
    "\n",
    "#On normalise : mettre entre 0 et 1\n",
    "def normalize(df, property, parameter):\n",
    "    df[property] = np.log(df[property] + parameter)\n",
    "\n",
    "\n",
    "normalize(df, \"fixed acidity\", -2.3)\n",
    "normalize(df, \"sulphates\", -0.24)\n",
    "normalize(df, \"total sulfur dioxide\", 5)\n",
    "normalize(df, \"residual sugar\", -1.1)\n",
    "normalize(df, \"chlorides\", -0.005)\n",
    "normalize(df, \"volatile acidity\", 2)\n",
    "normalize(df, \"free sulfur dioxide\", 2)\n",
    "#plot_hists(df)\n",
    "\n",
    "standardized = (df - df.mean()) / df.std()\n",
    "standardized = standardized[(np.abs(standardized) < 3).all(axis=1)]\n",
    "rows = np.setdiff1d(list(df.index), list(standardized.index))\n",
    "df.drop(index=rows, inplace=True)\n",
    "#plot_hists(df)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Préparation des données\n",
    "y = df['quality']\n",
    "X= [df['fixed acidity'],  df['volatile acidity']  ,df['citric acid']  ,df['residual sugar'],  df['chlorides'],df['free sulfur dioxide']  ,df['total sulfur dioxide'],  df['density']    ,df['pH'],  df['sulphates']]\n",
    "X=np.transpose(np.array(X))\n",
    "y=np.asarray(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_features = ['fixed acidity',  'volatile acidity'  ,'citric acid'  ,'residual sugar',  'chlorides','free sulfur dioxide'  ,'total sulfur dioxide',  'density'    ,'pH',  'sulphates']\n",
    "nb_feature=len(X_features)\n",
    "\n",
    "\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 2)\n",
    "supp=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]==5 or y[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.5):\n",
    "            supp.append(i)\n",
    "y2=np.delete(y,supp)\n",
    "\n",
    "X2=np.delete(X,supp,0)\n",
    "\n",
    "#Plot des modifications\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(y, bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality avant modification\")\n",
    "\n",
    "ax[1].hist(y2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality après modification\")\n",
    "\n",
    "#on créé les jeux de données\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X2, y2, test_size=0.4, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    " # normalize the original features\n",
    "X_train, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "X_cv, X_mu, X_sigma = zscore_normalize_features(X_cv)\n",
    "X_test, X_mu, X_sigma = zscore_normalize_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1- p)*np.log2(1 - p)\n",
    "    \n",
    "print(entropy(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def split_indices_continue(X,t, index_feature):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[0,index_feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29259344  1.94181282 -1.36502663 ...  0.03616459 -0.70892755\n",
      "   0.1308811 ]\n",
      " [-0.29259344  1.27349242 -1.16156762 ...  0.14010255 -0.32577481\n",
      "  -0.04525363]\n",
      " [-0.52157961  0.93933222 -1.36502663 ...  0.55585438  1.27069495\n",
      "  -0.57365783]\n",
      " ...\n",
      " [-1.4375243   0.43809192 -0.80551435 ... -0.9876243   1.52613011\n",
      "   0.01345795]\n",
      " [-0.86505887  0.49378528 -0.9581086  ... -0.11454545  0.69596584\n",
      "   0.95284319]\n",
      " [-1.38027776  0.10393172 -0.8563791  ... -0.83691426  1.33455374\n",
      "   0.60057372]]\n",
      "442 135\n"
     ]
    }
   ],
   "source": [
    "X_train=X_norm2\n",
    "y_train=y_train2\n",
    "left,right=split_indices_continue(X_train,0.5, 0)\n",
    "print(X_train)\n",
    "print(len(left),len(right))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut utiliser l'entrepie, mais on va utilsier le critère de gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X,y,left_indices,right_indices):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "    p_left = sum(y[left_indices])/len(left_indices)\n",
    "    print(p_left)\n",
    "    p_right = sum(y[right_indices])/len(right_indices)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "\n",
    "    #calcul p_left et p_right : moyenne de Y>threshold_y(qualité du vin) (moyenne d'un tableau de 1 0 1 00 1...)\n",
    "    p_left=np.mean(y[left_indices]>=threshold_y)\n",
    "    p_right=np.mean(y[right_indices]>=threshold_y)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   6.  33. 191. 188. 143.  16.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7186937755497419"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_indices, right_indices = split_indices_continue(X_train,0.5, 0)\n",
    "gini_Impurity(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node = sum(y)/len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X,y,left_indices,right_indices)\n",
    "    return h_node - w_entropy\n",
    "\n",
    "def information_gain_continue(X, y, left_indices, right_indices, threshold_y):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node=np.mean(y[left_indices]>=threshold_y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01885745298306668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain_continue(X_train, y_train, left_indices, right_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: fixed acidity, information gain if we split the root node using this feature: 0.02\n",
      "Feature: volatile acidity, information gain if we split the root node using this feature: -0.11\n",
      "Feature: citric acid, information gain if we split the root node using this feature: 0.07\n",
      "Feature: residual sugar, information gain if we split the root node using this feature: -0.02\n",
      "Feature: chlorides, information gain if we split the root node using this feature: -0.01\n",
      "Feature: free sulfur dioxide, information gain if we split the root node using this feature: 0.01\n",
      "Feature: total sulfur dioxide, information gain if we split the root node using this feature: 0.00\n",
      "Feature: density, information gain if we split the root node using this feature: -0.01\n",
      "Feature: pH, information gain if we split the root node using this feature: -0.05\n",
      "Feature: sulphates, information gain if we split the root node using this feature: 0.06\n"
     ]
    }
   ],
   "source": [
    "for i, feature_name in enumerate(X_features):\n",
    "    left_indices, right_indices = split_indices_continue(X_train, 0.5,i)\n",
    "    i_gain = information_gain_continue(X_train, y_train, left_indices, right_indices,5)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "\n",
    "    entropy = 0\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    entropy = sum(y[y==1])/len(y)\n",
    "    if entropy == 0 or entropy == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -entropy*np.log2(entropy) - (1-entropy)*np.log2(1-entropy)\n",
    "     \n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    for i in node_indices:\n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "    print(\"max_info_gain\",max_info_gain)\n",
    "   \n",
    "    return best_feature\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for multi classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    #print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "    \n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    #print(\"t\",t)\n",
    "    for i in node_indices:\n",
    "        #print(X[i,feature])\n",
    "        if X[i,feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "def compute_information_gain_continue(X, y, node_indices, feature, t):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,t)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = gini_Impurity(y_node)\n",
    "    left_entropy = gini_Impurity(y_left)\n",
    "    right_entropy = gini_Impurity(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    tmax=0\n",
    "\n",
    "    tab_max_feature=np.zeros(num_features)\n",
    "    tab_min_feature=np.zeros(num_features)\n",
    "    for loop in range(num_features):\n",
    "        tab_max_feature[loop]=np.max(np.transpose(X)[loop])\n",
    "        tab_min_feature[loop]=np.min(np.transpose(X)[loop])\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        tab_t_feature=np.linspace(tab_min_feature[feature], tab_max_feature[feature], len(X)-1)\n",
    "        for t in range(len(tab_t_feature)):\n",
    "            #print(t,X_features[feature])\n",
    "            info_gain = compute_information_gain_continue(X, y, node_indices, feature,tab_t_feature[t])\n",
    "            #print(tab_t_feature[t],X_features[feature],info_gain)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                tmax=tab_t_feature[t]\n",
    "    #print(\"max_info_gain\",max_info_gain,X_features[best_feature],tmax)\n",
    "   \n",
    "    return best_feature,tmax,max_info_gain\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        print(formatting,\"note moyenne attribuée à la feuille :\",np.mean(y[node_indices]),\"(\",round(np.mean(y[node_indices])),\")\")\n",
    "        return 0\n",
    "   \n",
    "\n",
    "    best_feature,tmax,max_info = get_best_split_continue(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %s <= %s, pour un gain de %s\" % (formatting, current_depth, branch_name, X_features[best_feature], tmax,max_info))\n",
    "\n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, best_feature,tmax)\n",
    "    tree.append((left_indices, right_indices, best_feature,tmax))\n",
    "    \n",
    "    build_tree_recursive_continue(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive_continue(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29259344  1.94181282 -1.36502663 ...  0.03616459 -0.70892755\n",
      "   0.1308811 ]\n",
      " [-0.29259344  1.27349242 -1.16156762 ...  0.14010255 -0.32577481\n",
      "  -0.04525363]\n",
      " [-0.52157961  0.93933222 -1.36502663 ...  0.55585438  1.27069495\n",
      "  -0.57365783]\n",
      " ...\n",
      " [-1.4375243   0.43809192 -0.80551435 ... -0.9876243   1.52613011\n",
      "   0.01345795]\n",
      " [-0.86505887  0.49378528 -0.9581086  ... -0.11454545  0.69596584\n",
      "   0.95284319]\n",
      " [-1.38027776  0.10393172 -0.8563791  ... -0.83691426  1.33455374\n",
      "   0.60057372]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47022/1131864183.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  impurity-=(tab_value[loop]/sum(tab_value))**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: sulphates <= -0.2188356857167224, pour un gain de 0.05171474670144571\n",
      "- Depth 1, Left: Split on feature: density <= -0.9005423639943579, pour un gain de 0.04386336762114973\n",
      "-- Depth 2, Left: Split on feature: volatile acidity <= -0.9474621927873392, pour un gain de 0.1371527777777778\n",
      "   --- Left leaf node with indices [129, 206, 289, 290, 323, 327, 330, 346, 350, 359, 378, 379, 399, 425, 449, 530]\n",
      "   --- note moyenne attribuée à la feuille : 6.8125 ( 7 )\n",
      "   --- Right leaf node with indices [5, 18, 31, 72, 81, 153, 212, 260, 281, 313, 337, 356, 362, 371, 387, 391, 396, 397, 418, 435, 437, 443, 454, 455, 456, 462, 463, 467, 524, 529, 551, 571]\n",
      "   --- note moyenne attribuée à la feuille : 5.375 ( 5 )\n",
      "-- Depth 2, Right: Split on feature: total sulfur dioxide <= 1.0799603516275635, pour un gain de 0.04476930596910189\n",
      "   --- Left leaf node with indices [2, 3, 4, 6, 8, 10, 11, 13, 14, 16, 22, 23, 25, 26, 28, 30, 32, 33, 34, 35, 36, 37, 40, 41, 42, 47, 50, 57, 59, 61, 62, 63, 65, 66, 67, 68, 71, 78, 79, 80, 82, 88, 93, 94, 112, 114, 133, 142, 144, 147, 149, 150, 152, 154, 156, 160, 164, 168, 170, 171, 191, 192, 193, 197, 201, 205, 208, 209, 214, 215, 216, 217, 222, 224, 226, 227, 229, 233, 234, 236, 237, 238, 240, 243, 247, 248, 250, 251, 253, 254, 255, 259, 261, 262, 267, 269, 270, 271, 279, 282, 286, 287, 299, 300, 303, 308, 312, 314, 315, 316, 321, 333, 336, 340, 341, 342, 343, 344, 351, 352, 354, 355, 357, 364, 367, 374, 377, 381, 389, 395, 406, 408, 414, 416, 421, 422, 423, 427, 436, 438, 439, 440, 445, 446, 447, 448, 452, 453, 460, 464, 465, 468, 469, 470, 471, 473, 475, 477, 481, 487, 497, 498, 502, 512, 513, 515, 517, 518, 522, 526, 527, 536, 545, 555, 556, 559, 563, 564, 566, 570]\n",
      "   --- note moyenne attribuée à la feuille : 5.305555555555555 ( 5 )\n",
      "   --- Right leaf node with indices [12, 49, 52, 53, 60, 69, 70, 87, 116, 143, 213, 220, 221, 239, 244, 249, 258, 263, 264, 265, 266, 268, 273, 293, 298, 306, 365, 376, 404, 458, 459, 472, 478, 479, 480, 484, 485, 510, 550, 561, 562, 568, 572]\n",
      "   --- note moyenne attribuée à la feuille : 5.046511627906977 ( 5 )\n",
      "- Depth 1, Right: Split on feature: citric acid <= 0.21841525059419764, pour un gain de 0.040485752735767266\n",
      "-- Depth 2, Left: Split on feature: free sulfur dioxide <= 0.9201631534765737, pour un gain de 0.046511808742681016\n",
      "   --- Left leaf node with indices [0, 1, 9, 15, 17, 19, 20, 24, 29, 39, 43, 58, 64, 76, 83, 86, 89, 92, 95, 102, 109, 110, 111, 115, 121, 127, 128, 141, 159, 175, 177, 194, 195, 198, 202, 211, 223, 228, 231, 232, 242, 245, 252, 256, 257, 274, 275, 277, 278, 283, 284, 292, 302, 304, 305, 310, 311, 334, 335, 338, 345, 361, 372, 393, 400, 402, 405, 410, 417, 419, 420, 433, 434, 441, 442, 451, 457, 466, 474, 482, 483, 493, 494, 500, 503, 505, 520, 521, 528, 531, 532, 533, 534, 537, 538, 541, 544, 547, 548, 549, 552, 557, 560, 574]\n",
      "   --- note moyenne attribuée à la feuille : 5.9326923076923075 ( 6 )\n",
      "   --- Right leaf node with indices [21, 27, 46, 48, 97, 100, 101, 108, 118, 184, 318, 319, 415, 444, 450, 490, 499, 508, 542, 558, 565, 575, 576]\n",
      "   --- note moyenne attribuée à la feuille : 5.869565217391305 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: total sulfur dioxide <= 1.755862762604987, pour un gain de 0.03807116351827411\n",
      "   --- Left leaf node with indices [7, 38, 45, 51, 73, 74, 75, 84, 85, 90, 91, 96, 98, 99, 103, 104, 105, 106, 107, 113, 117, 119, 120, 122, 123, 124, 125, 126, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 145, 146, 148, 151, 155, 157, 158, 161, 162, 163, 165, 166, 167, 169, 172, 173, 174, 176, 178, 179, 180, 181, 182, 183, 185, 186, 188, 189, 190, 196, 199, 200, 203, 204, 207, 210, 218, 219, 225, 230, 235, 241, 246, 272, 276, 280, 285, 288, 291, 294, 295, 296, 297, 301, 307, 309, 317, 320, 322, 324, 325, 326, 328, 329, 331, 332, 339, 347, 348, 349, 353, 358, 360, 363, 366, 368, 369, 370, 373, 375, 380, 382, 383, 384, 385, 386, 388, 390, 392, 394, 398, 401, 403, 407, 409, 411, 412, 413, 424, 426, 428, 429, 430, 431, 432, 461, 476, 486, 488, 489, 491, 492, 495, 496, 501, 504, 506, 507, 509, 511, 514, 516, 519, 523, 525, 535, 539, 540, 543, 546, 553, 554, 567, 569, 573]\n",
      "   --- note moyenne attribuée à la feuille : 6.514450867052023 ( 7 )\n",
      "   --- Right leaf node with indices [44, 54, 55, 56, 77, 187]\n",
      "   --- note moyenne attribuée à la feuille : 5.0 ( 5 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   47,\n",
       "   49,\n",
       "   50,\n",
       "   52,\n",
       "   53,\n",
       "   57,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   87,\n",
       "   88,\n",
       "   93,\n",
       "   94,\n",
       "   112,\n",
       "   114,\n",
       "   116,\n",
       "   129,\n",
       "   133,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   147,\n",
       "   149,\n",
       "   150,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   156,\n",
       "   160,\n",
       "   164,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   206,\n",
       "   208,\n",
       "   209,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   224,\n",
       "   226,\n",
       "   227,\n",
       "   229,\n",
       "   233,\n",
       "   234,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   243,\n",
       "   244,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   273,\n",
       "   279,\n",
       "   281,\n",
       "   282,\n",
       "   286,\n",
       "   287,\n",
       "   289,\n",
       "   290,\n",
       "   293,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   303,\n",
       "   306,\n",
       "   308,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   321,\n",
       "   323,\n",
       "   327,\n",
       "   330,\n",
       "   333,\n",
       "   336,\n",
       "   337,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   346,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   354,\n",
       "   355,\n",
       "   356,\n",
       "   357,\n",
       "   359,\n",
       "   362,\n",
       "   364,\n",
       "   365,\n",
       "   367,\n",
       "   371,\n",
       "   374,\n",
       "   376,\n",
       "   377,\n",
       "   378,\n",
       "   379,\n",
       "   381,\n",
       "   387,\n",
       "   389,\n",
       "   391,\n",
       "   395,\n",
       "   396,\n",
       "   397,\n",
       "   399,\n",
       "   404,\n",
       "   406,\n",
       "   408,\n",
       "   414,\n",
       "   416,\n",
       "   418,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   425,\n",
       "   427,\n",
       "   435,\n",
       "   436,\n",
       "   437,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   443,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   449,\n",
       "   452,\n",
       "   453,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   462,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   467,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   475,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   484,\n",
       "   485,\n",
       "   487,\n",
       "   497,\n",
       "   498,\n",
       "   502,\n",
       "   510,\n",
       "   512,\n",
       "   513,\n",
       "   515,\n",
       "   517,\n",
       "   518,\n",
       "   522,\n",
       "   524,\n",
       "   526,\n",
       "   527,\n",
       "   529,\n",
       "   530,\n",
       "   536,\n",
       "   545,\n",
       "   550,\n",
       "   551,\n",
       "   555,\n",
       "   556,\n",
       "   559,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   566,\n",
       "   568,\n",
       "   570,\n",
       "   571,\n",
       "   572],\n",
       "  [0,\n",
       "   1,\n",
       "   7,\n",
       "   9,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   27,\n",
       "   29,\n",
       "   38,\n",
       "   39,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   48,\n",
       "   51,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   58,\n",
       "   64,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   113,\n",
       "   115,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   145,\n",
       "   146,\n",
       "   148,\n",
       "   151,\n",
       "   155,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   169,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   207,\n",
       "   210,\n",
       "   211,\n",
       "   218,\n",
       "   219,\n",
       "   223,\n",
       "   225,\n",
       "   228,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   235,\n",
       "   241,\n",
       "   242,\n",
       "   245,\n",
       "   246,\n",
       "   252,\n",
       "   256,\n",
       "   257,\n",
       "   272,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   280,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   288,\n",
       "   291,\n",
       "   292,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   301,\n",
       "   302,\n",
       "   304,\n",
       "   305,\n",
       "   307,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   322,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   328,\n",
       "   329,\n",
       "   331,\n",
       "   332,\n",
       "   334,\n",
       "   335,\n",
       "   338,\n",
       "   339,\n",
       "   345,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   353,\n",
       "   358,\n",
       "   360,\n",
       "   361,\n",
       "   363,\n",
       "   366,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   372,\n",
       "   373,\n",
       "   375,\n",
       "   380,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   388,\n",
       "   390,\n",
       "   392,\n",
       "   393,\n",
       "   394,\n",
       "   398,\n",
       "   400,\n",
       "   401,\n",
       "   402,\n",
       "   403,\n",
       "   405,\n",
       "   407,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   415,\n",
       "   417,\n",
       "   419,\n",
       "   420,\n",
       "   424,\n",
       "   426,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   433,\n",
       "   434,\n",
       "   441,\n",
       "   442,\n",
       "   444,\n",
       "   450,\n",
       "   451,\n",
       "   457,\n",
       "   461,\n",
       "   466,\n",
       "   474,\n",
       "   476,\n",
       "   482,\n",
       "   483,\n",
       "   486,\n",
       "   488,\n",
       "   489,\n",
       "   490,\n",
       "   491,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   503,\n",
       "   504,\n",
       "   505,\n",
       "   506,\n",
       "   507,\n",
       "   508,\n",
       "   509,\n",
       "   511,\n",
       "   514,\n",
       "   516,\n",
       "   519,\n",
       "   520,\n",
       "   521,\n",
       "   523,\n",
       "   525,\n",
       "   528,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   535,\n",
       "   537,\n",
       "   538,\n",
       "   539,\n",
       "   540,\n",
       "   541,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   546,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   557,\n",
       "   558,\n",
       "   560,\n",
       "   565,\n",
       "   567,\n",
       "   569,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   576],\n",
       "  9,\n",
       "  -0.2188356857167224),\n",
       " ([5,\n",
       "   18,\n",
       "   31,\n",
       "   72,\n",
       "   81,\n",
       "   129,\n",
       "   153,\n",
       "   206,\n",
       "   212,\n",
       "   260,\n",
       "   281,\n",
       "   289,\n",
       "   290,\n",
       "   313,\n",
       "   323,\n",
       "   327,\n",
       "   330,\n",
       "   337,\n",
       "   346,\n",
       "   350,\n",
       "   356,\n",
       "   359,\n",
       "   362,\n",
       "   371,\n",
       "   378,\n",
       "   379,\n",
       "   387,\n",
       "   391,\n",
       "   396,\n",
       "   397,\n",
       "   399,\n",
       "   418,\n",
       "   425,\n",
       "   435,\n",
       "   437,\n",
       "   443,\n",
       "   449,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   462,\n",
       "   463,\n",
       "   467,\n",
       "   524,\n",
       "   529,\n",
       "   530,\n",
       "   551,\n",
       "   571],\n",
       "  [2,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   16,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   47,\n",
       "   49,\n",
       "   50,\n",
       "   52,\n",
       "   53,\n",
       "   57,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   82,\n",
       "   87,\n",
       "   88,\n",
       "   93,\n",
       "   94,\n",
       "   112,\n",
       "   114,\n",
       "   116,\n",
       "   133,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   147,\n",
       "   149,\n",
       "   150,\n",
       "   152,\n",
       "   154,\n",
       "   156,\n",
       "   160,\n",
       "   164,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   208,\n",
       "   209,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   224,\n",
       "   226,\n",
       "   227,\n",
       "   229,\n",
       "   233,\n",
       "   234,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   243,\n",
       "   244,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   258,\n",
       "   259,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   273,\n",
       "   279,\n",
       "   282,\n",
       "   286,\n",
       "   287,\n",
       "   293,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   303,\n",
       "   306,\n",
       "   308,\n",
       "   312,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   321,\n",
       "   333,\n",
       "   336,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   351,\n",
       "   352,\n",
       "   354,\n",
       "   355,\n",
       "   357,\n",
       "   364,\n",
       "   365,\n",
       "   367,\n",
       "   374,\n",
       "   376,\n",
       "   377,\n",
       "   381,\n",
       "   389,\n",
       "   395,\n",
       "   404,\n",
       "   406,\n",
       "   408,\n",
       "   414,\n",
       "   416,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   427,\n",
       "   436,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   452,\n",
       "   453,\n",
       "   458,\n",
       "   459,\n",
       "   460,\n",
       "   464,\n",
       "   465,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   472,\n",
       "   473,\n",
       "   475,\n",
       "   477,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   481,\n",
       "   484,\n",
       "   485,\n",
       "   487,\n",
       "   497,\n",
       "   498,\n",
       "   502,\n",
       "   510,\n",
       "   512,\n",
       "   513,\n",
       "   515,\n",
       "   517,\n",
       "   518,\n",
       "   522,\n",
       "   526,\n",
       "   527,\n",
       "   536,\n",
       "   545,\n",
       "   550,\n",
       "   555,\n",
       "   556,\n",
       "   559,\n",
       "   561,\n",
       "   562,\n",
       "   563,\n",
       "   564,\n",
       "   566,\n",
       "   568,\n",
       "   570,\n",
       "   572],\n",
       "  7,\n",
       "  -0.9005423639943579),\n",
       " ([129,\n",
       "   206,\n",
       "   289,\n",
       "   290,\n",
       "   323,\n",
       "   327,\n",
       "   330,\n",
       "   346,\n",
       "   350,\n",
       "   359,\n",
       "   378,\n",
       "   379,\n",
       "   399,\n",
       "   425,\n",
       "   449,\n",
       "   530],\n",
       "  [5,\n",
       "   18,\n",
       "   31,\n",
       "   72,\n",
       "   81,\n",
       "   153,\n",
       "   212,\n",
       "   260,\n",
       "   281,\n",
       "   313,\n",
       "   337,\n",
       "   356,\n",
       "   362,\n",
       "   371,\n",
       "   387,\n",
       "   391,\n",
       "   396,\n",
       "   397,\n",
       "   418,\n",
       "   435,\n",
       "   437,\n",
       "   443,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   462,\n",
       "   463,\n",
       "   467,\n",
       "   524,\n",
       "   529,\n",
       "   551,\n",
       "   571],\n",
       "  1,\n",
       "  -0.9474621927873392),\n",
       " ([2,\n",
       "   3,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   11,\n",
       "   13,\n",
       "   14,\n",
       "   16,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   47,\n",
       "   50,\n",
       "   57,\n",
       "   59,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   71,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   82,\n",
       "   88,\n",
       "   93,\n",
       "   94,\n",
       "   112,\n",
       "   114,\n",
       "   133,\n",
       "   142,\n",
       "   144,\n",
       "   147,\n",
       "   149,\n",
       "   150,\n",
       "   152,\n",
       "   154,\n",
       "   156,\n",
       "   160,\n",
       "   164,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   197,\n",
       "   201,\n",
       "   205,\n",
       "   208,\n",
       "   209,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   222,\n",
       "   224,\n",
       "   226,\n",
       "   227,\n",
       "   229,\n",
       "   233,\n",
       "   234,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   240,\n",
       "   243,\n",
       "   247,\n",
       "   248,\n",
       "   250,\n",
       "   251,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   259,\n",
       "   261,\n",
       "   262,\n",
       "   267,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   279,\n",
       "   282,\n",
       "   286,\n",
       "   287,\n",
       "   299,\n",
       "   300,\n",
       "   303,\n",
       "   308,\n",
       "   312,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   321,\n",
       "   333,\n",
       "   336,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   351,\n",
       "   352,\n",
       "   354,\n",
       "   355,\n",
       "   357,\n",
       "   364,\n",
       "   367,\n",
       "   374,\n",
       "   377,\n",
       "   381,\n",
       "   389,\n",
       "   395,\n",
       "   406,\n",
       "   408,\n",
       "   414,\n",
       "   416,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   427,\n",
       "   436,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   445,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   452,\n",
       "   453,\n",
       "   460,\n",
       "   464,\n",
       "   465,\n",
       "   468,\n",
       "   469,\n",
       "   470,\n",
       "   471,\n",
       "   473,\n",
       "   475,\n",
       "   477,\n",
       "   481,\n",
       "   487,\n",
       "   497,\n",
       "   498,\n",
       "   502,\n",
       "   512,\n",
       "   513,\n",
       "   515,\n",
       "   517,\n",
       "   518,\n",
       "   522,\n",
       "   526,\n",
       "   527,\n",
       "   536,\n",
       "   545,\n",
       "   555,\n",
       "   556,\n",
       "   559,\n",
       "   563,\n",
       "   564,\n",
       "   566,\n",
       "   570],\n",
       "  [12,\n",
       "   49,\n",
       "   52,\n",
       "   53,\n",
       "   60,\n",
       "   69,\n",
       "   70,\n",
       "   87,\n",
       "   116,\n",
       "   143,\n",
       "   213,\n",
       "   220,\n",
       "   221,\n",
       "   239,\n",
       "   244,\n",
       "   249,\n",
       "   258,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   268,\n",
       "   273,\n",
       "   293,\n",
       "   298,\n",
       "   306,\n",
       "   365,\n",
       "   376,\n",
       "   404,\n",
       "   458,\n",
       "   459,\n",
       "   472,\n",
       "   478,\n",
       "   479,\n",
       "   480,\n",
       "   484,\n",
       "   485,\n",
       "   510,\n",
       "   550,\n",
       "   561,\n",
       "   562,\n",
       "   568,\n",
       "   572],\n",
       "  6,\n",
       "  1.0799603516275635),\n",
       " ([0,\n",
       "   1,\n",
       "   9,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   27,\n",
       "   29,\n",
       "   39,\n",
       "   43,\n",
       "   46,\n",
       "   48,\n",
       "   58,\n",
       "   64,\n",
       "   76,\n",
       "   83,\n",
       "   86,\n",
       "   89,\n",
       "   92,\n",
       "   95,\n",
       "   97,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   115,\n",
       "   118,\n",
       "   121,\n",
       "   127,\n",
       "   128,\n",
       "   141,\n",
       "   159,\n",
       "   175,\n",
       "   177,\n",
       "   184,\n",
       "   194,\n",
       "   195,\n",
       "   198,\n",
       "   202,\n",
       "   211,\n",
       "   223,\n",
       "   228,\n",
       "   231,\n",
       "   232,\n",
       "   242,\n",
       "   245,\n",
       "   252,\n",
       "   256,\n",
       "   257,\n",
       "   274,\n",
       "   275,\n",
       "   277,\n",
       "   278,\n",
       "   283,\n",
       "   284,\n",
       "   292,\n",
       "   302,\n",
       "   304,\n",
       "   305,\n",
       "   310,\n",
       "   311,\n",
       "   318,\n",
       "   319,\n",
       "   334,\n",
       "   335,\n",
       "   338,\n",
       "   345,\n",
       "   361,\n",
       "   372,\n",
       "   393,\n",
       "   400,\n",
       "   402,\n",
       "   405,\n",
       "   410,\n",
       "   415,\n",
       "   417,\n",
       "   419,\n",
       "   420,\n",
       "   433,\n",
       "   434,\n",
       "   441,\n",
       "   442,\n",
       "   444,\n",
       "   450,\n",
       "   451,\n",
       "   457,\n",
       "   466,\n",
       "   474,\n",
       "   482,\n",
       "   483,\n",
       "   490,\n",
       "   493,\n",
       "   494,\n",
       "   499,\n",
       "   500,\n",
       "   503,\n",
       "   505,\n",
       "   508,\n",
       "   520,\n",
       "   521,\n",
       "   528,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   537,\n",
       "   538,\n",
       "   541,\n",
       "   542,\n",
       "   544,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   552,\n",
       "   557,\n",
       "   558,\n",
       "   560,\n",
       "   565,\n",
       "   574,\n",
       "   575,\n",
       "   576],\n",
       "  [7,\n",
       "   38,\n",
       "   44,\n",
       "   45,\n",
       "   51,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   77,\n",
       "   84,\n",
       "   85,\n",
       "   90,\n",
       "   91,\n",
       "   96,\n",
       "   98,\n",
       "   99,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   113,\n",
       "   117,\n",
       "   119,\n",
       "   120,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   145,\n",
       "   146,\n",
       "   148,\n",
       "   151,\n",
       "   155,\n",
       "   157,\n",
       "   158,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   169,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   176,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   196,\n",
       "   199,\n",
       "   200,\n",
       "   203,\n",
       "   204,\n",
       "   207,\n",
       "   210,\n",
       "   218,\n",
       "   219,\n",
       "   225,\n",
       "   230,\n",
       "   235,\n",
       "   241,\n",
       "   246,\n",
       "   272,\n",
       "   276,\n",
       "   280,\n",
       "   285,\n",
       "   288,\n",
       "   291,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   301,\n",
       "   307,\n",
       "   309,\n",
       "   317,\n",
       "   320,\n",
       "   322,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   328,\n",
       "   329,\n",
       "   331,\n",
       "   332,\n",
       "   339,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   353,\n",
       "   358,\n",
       "   360,\n",
       "   363,\n",
       "   366,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   373,\n",
       "   375,\n",
       "   380,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   388,\n",
       "   390,\n",
       "   392,\n",
       "   394,\n",
       "   398,\n",
       "   401,\n",
       "   403,\n",
       "   407,\n",
       "   409,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   424,\n",
       "   426,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   461,\n",
       "   476,\n",
       "   486,\n",
       "   488,\n",
       "   489,\n",
       "   491,\n",
       "   492,\n",
       "   495,\n",
       "   496,\n",
       "   501,\n",
       "   504,\n",
       "   506,\n",
       "   507,\n",
       "   509,\n",
       "   511,\n",
       "   514,\n",
       "   516,\n",
       "   519,\n",
       "   523,\n",
       "   525,\n",
       "   535,\n",
       "   539,\n",
       "   540,\n",
       "   543,\n",
       "   546,\n",
       "   553,\n",
       "   554,\n",
       "   567,\n",
       "   569,\n",
       "   573],\n",
       "  2,\n",
       "  0.21841525059419764),\n",
       " ([0,\n",
       "   1,\n",
       "   9,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   24,\n",
       "   29,\n",
       "   39,\n",
       "   43,\n",
       "   58,\n",
       "   64,\n",
       "   76,\n",
       "   83,\n",
       "   86,\n",
       "   89,\n",
       "   92,\n",
       "   95,\n",
       "   102,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   115,\n",
       "   121,\n",
       "   127,\n",
       "   128,\n",
       "   141,\n",
       "   159,\n",
       "   175,\n",
       "   177,\n",
       "   194,\n",
       "   195,\n",
       "   198,\n",
       "   202,\n",
       "   211,\n",
       "   223,\n",
       "   228,\n",
       "   231,\n",
       "   232,\n",
       "   242,\n",
       "   245,\n",
       "   252,\n",
       "   256,\n",
       "   257,\n",
       "   274,\n",
       "   275,\n",
       "   277,\n",
       "   278,\n",
       "   283,\n",
       "   284,\n",
       "   292,\n",
       "   302,\n",
       "   304,\n",
       "   305,\n",
       "   310,\n",
       "   311,\n",
       "   334,\n",
       "   335,\n",
       "   338,\n",
       "   345,\n",
       "   361,\n",
       "   372,\n",
       "   393,\n",
       "   400,\n",
       "   402,\n",
       "   405,\n",
       "   410,\n",
       "   417,\n",
       "   419,\n",
       "   420,\n",
       "   433,\n",
       "   434,\n",
       "   441,\n",
       "   442,\n",
       "   451,\n",
       "   457,\n",
       "   466,\n",
       "   474,\n",
       "   482,\n",
       "   483,\n",
       "   493,\n",
       "   494,\n",
       "   500,\n",
       "   503,\n",
       "   505,\n",
       "   520,\n",
       "   521,\n",
       "   528,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   537,\n",
       "   538,\n",
       "   541,\n",
       "   544,\n",
       "   547,\n",
       "   548,\n",
       "   549,\n",
       "   552,\n",
       "   557,\n",
       "   560,\n",
       "   574],\n",
       "  [21,\n",
       "   27,\n",
       "   46,\n",
       "   48,\n",
       "   97,\n",
       "   100,\n",
       "   101,\n",
       "   108,\n",
       "   118,\n",
       "   184,\n",
       "   318,\n",
       "   319,\n",
       "   415,\n",
       "   444,\n",
       "   450,\n",
       "   490,\n",
       "   499,\n",
       "   508,\n",
       "   542,\n",
       "   558,\n",
       "   565,\n",
       "   575,\n",
       "   576],\n",
       "  5,\n",
       "  0.9201631534765737),\n",
       " ([7,\n",
       "   38,\n",
       "   45,\n",
       "   51,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   84,\n",
       "   85,\n",
       "   90,\n",
       "   91,\n",
       "   96,\n",
       "   98,\n",
       "   99,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   113,\n",
       "   117,\n",
       "   119,\n",
       "   120,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   145,\n",
       "   146,\n",
       "   148,\n",
       "   151,\n",
       "   155,\n",
       "   157,\n",
       "   158,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   169,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   176,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   185,\n",
       "   186,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   196,\n",
       "   199,\n",
       "   200,\n",
       "   203,\n",
       "   204,\n",
       "   207,\n",
       "   210,\n",
       "   218,\n",
       "   219,\n",
       "   225,\n",
       "   230,\n",
       "   235,\n",
       "   241,\n",
       "   246,\n",
       "   272,\n",
       "   276,\n",
       "   280,\n",
       "   285,\n",
       "   288,\n",
       "   291,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   301,\n",
       "   307,\n",
       "   309,\n",
       "   317,\n",
       "   320,\n",
       "   322,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   328,\n",
       "   329,\n",
       "   331,\n",
       "   332,\n",
       "   339,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   353,\n",
       "   358,\n",
       "   360,\n",
       "   363,\n",
       "   366,\n",
       "   368,\n",
       "   369,\n",
       "   370,\n",
       "   373,\n",
       "   375,\n",
       "   380,\n",
       "   382,\n",
       "   383,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   388,\n",
       "   390,\n",
       "   392,\n",
       "   394,\n",
       "   398,\n",
       "   401,\n",
       "   403,\n",
       "   407,\n",
       "   409,\n",
       "   411,\n",
       "   412,\n",
       "   413,\n",
       "   424,\n",
       "   426,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   431,\n",
       "   432,\n",
       "   461,\n",
       "   476,\n",
       "   486,\n",
       "   488,\n",
       "   489,\n",
       "   491,\n",
       "   492,\n",
       "   495,\n",
       "   496,\n",
       "   501,\n",
       "   504,\n",
       "   506,\n",
       "   507,\n",
       "   509,\n",
       "   511,\n",
       "   514,\n",
       "   516,\n",
       "   519,\n",
       "   523,\n",
       "   525,\n",
       "   535,\n",
       "   539,\n",
       "   540,\n",
       "   543,\n",
       "   546,\n",
       "   553,\n",
       "   554,\n",
       "   567,\n",
       "   569,\n",
       "   573],\n",
       "  [44, 54, 55, 56, 77, 187],\n",
       "  6,\n",
       "  1.755862762604987)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = []\n",
    "print(X_train)\n",
    "root_indices=list(range(0, len(X_train)))\n",
    "build_tree_recursive_continue(X_train, y_train,root_indices, \"Root\", max_depth=3, current_depth=0, tree = tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passage du decision tree pour une classification multiple à random forest : à écrire..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREE ENSEMBLES : RANDOM FOREST POUR LA QUALITE DU VIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[-0.52157961  0.93933222 -1.36502663 ...  0.55585438  1.27069495\n",
      "  -0.57365783]\n",
      " [-0.29259344  1.94181282 -1.36502663 ...  0.03616459 -0.70892755\n",
      "   0.1308811 ]\n",
      " [-0.29259344  1.27349242 -1.16156762 ...  0.14010255 -0.32577481\n",
      "  -0.04525363]\n",
      " ...\n",
      " [-1.20853813  0.38239855 -0.9581086  ... -0.95124601  0.88754221\n",
      "  -0.45623467]\n",
      " [-1.38027776  0.10393172 -0.8563791  ... -0.83691426  1.33455374\n",
      "   0.60057372]\n",
      " [-1.38027776  0.6330187  -0.75464959 ... -0.65502283  1.65384769\n",
      "   0.30701583]]\n",
      "(574,)\n",
      "(574, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['fixed acidity'],  df['volatile acidity']  ,df['citric acid']  ,df['residual sugar'],  df['chlorides'],df['free sulfur dioxide']  ,df['total sulfur dioxide'],  df['density']    ,df['pH'],  df['sulphates']]\n",
    "X_features = ['fixed acidity',  'volatile acidity'  ,'citric acid'  ,'residual sugar',  'chlorides','free sulfur dioxide'  ,'total sulfur dioxide',  'density'    ,'pH',  'sulphates']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "nb_feature=len(X_features)\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2.shape)\n",
    "print(X_norm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459\n",
      "[5 7 7 5 7 5 5 5 6 5 5 6 7 5 4 6 4 5 5 5 5 4 5 4 6 5 5 4 5 6 6 5 5 5 5 6 5\n",
      " 6 5 5 7 5 6 5 4 5 5 5 4 5 5 4 6 4 5 5 5 5 6 5 6 5 6 4 7 7 7 6 5 6 5 5 6 6\n",
      " 7 7 7 6 7 4 5 5 4 8 6 6 6 6 8 7 7 7 5 7 7 5 5 5 6 5 5 6 6 6 7 6 5 7 6 5 7\n",
      " 7 6 7 6 6 6 5 6 5 7 7 6 5 6 7 7 7 7 8 5 7 5 5 6 7 4 5 7 5 6 7 5 7 7 6 5 7\n",
      " 6 5 6 6 8 7 7 5 6 6 7 5 8 5 3 5 6 6 5 6 8 5 6 7 7 8 6 5 8 6 6 7 7 7 7 7 7\n",
      " 6 7 7 6 3 6 7 6 6 6 6 5 6 5 5 6 4 5 5 7 5 8 5 6 5 5 5 5 6 6 5 4 5 5 5 4 7\n",
      " 5 5 7 6 6 5 6 5 5 5 5 5 5 5 5 6 6 6 4 4 5 5 5 5 5 5 5 6 5 5 5 6 5 6 6 5 5\n",
      " 5 6 6 5 5 6 5 5 5 6 5 5 7 7 6 7 5 5 6 4 5 7 5 7 4 4 7 7 7 5 6 6 7 7 5 5 4\n",
      " 7 6 6 6 5 5 5 6 6 7 6 7 7 7 7 6 6 6 6 5 5 7 6 4 5 7 5 5 6 7 7 7 7 7 7 7 7\n",
      " 7 7 7 6 5 5 6 6 7 5 7 5 5 5 6 6 7 7 7 7 7 7 6 6 7 6 5 6 6 7 5 7 5 7 7 7 6\n",
      " 6 6 6 5 7 6 7 7 7 6 8 6 7 7 5 7 6 7 7 6 7 6 7 7 8 7 6 7 5 6 6 7 6 6 6 6 6\n",
      " 6 5 8 7 6 7 7 7 6 6 7 6 7 7 7 5 7 6 4 5 4 5 7 6 6 7 8 7 7 5 7 7 6 6 6 6 5\n",
      " 5 7 5 6 6 6 4 4 6 5 6 4 6 6 6 6 5 4 7 6 5 5 6 6 3 6 6 6 5 5 5 6 6 5 6 6 6\n",
      " 6 6 6 6 5 5 6 6 5 6 5 6 6 5 6 5 5 6 5 5 6 5 6 6 5 8 7 6 6 6 5 5 7 5 4 5 5\n",
      " 7 5 5 5 5 6 5 8 7 7 7 6 6 5 7 4 7 4 7 3 5 7 7 3 4 5 5 5 7 6 5 6 3 6 5 7 6\n",
      " 6 7 5 8 5 7 6 5 5 6 5 6 5 6 5 7 6 5 5]\n",
      "[2 4 4 2 4 2 2 2 3 2 2 3 4 2 1 3 1 2 2 2 2 1 2 1 3 2 2 1 2 3 3 2 2 2 2 3 2\n",
      " 3 2 2 4 2 3 2 1 2 2 2 1 2 2 1 3 1 2 2 2 2 3 2 3 2 3 1 4 4 4 3 2 3 2 2 3 3\n",
      " 4 4 4 3 4 1 2 2 1 5 3 3 3 3 5 4 4 4 2 4 4 2 2 2 3 2 2 3 3 3 4 3 2 4 3 2 4\n",
      " 4 3 4 3 3 3 2 3 2 4 4 3 2 3 4 4 4 4 5 2 4 2 2 3 4 1 2 4 2 3 4 2 4 4 3 2 4\n",
      " 3 2 3 3 5 4 4 2 3 3 4 2 5 2 0 2 3 3 2 3 5 2 3 4 4 5 3 2 5 3 3 4 4 4 4 4 4\n",
      " 3 4 4 3 0 3 4 3 3 3 3 2 3 2 2 3 1 2 2 4 2 5 2 3 2 2 2 2 3 3 2 1 2 2 2 1 4\n",
      " 2 2 4 3 3 2 3 2 2 2 2 2 2 2 2 3 3 3 1 1 2 2 2 2 2 2 2 3 2 2 2 3 2 3 3 2 2\n",
      " 2 3 3 2 2 3 2 2 2 3 2 2 4 4 3 4 2 2 3 1 2 4 2 4 1 1 4 4 4 2 3 3 4 4 2 2 1\n",
      " 4 3 3 3 2 2 2 3 3 4 3 4 4 4 4 3 3 3 3 2 2 4 3 1 2 4 2 2 3 4 4 4 4 4 4 4 4\n",
      " 4 4 4 3 2 2 3 3 4 2 4 2 2 2 3 3 4 4 4 4 4 4 3 3 4 3 2 3 3 4 2 4 2 4 4 4 3\n",
      " 3 3 3 2 4 3 4 4 4 3 5 3 4 4 2 4 3 4 4 3 4 3 4 4 5 4 3 4 2 3 3 4 3 3 3 3 3\n",
      " 3 2 5 4 3 4 4 4 3 3 4 3 4 4 4 2 4 3 1 2 1 2 4 3 3 4 5 4 4 2 4 4 3 3 3 3 2\n",
      " 2 4 2 3 3 3 1 1 3 2 3 1 3 3 3 3 2 1 4 3 2 2 3 3 0 3 3 3 2 2 2 3 3 2 3 3 3\n",
      " 3 3 3 3 2 2 3 3 2 3 2 3 3 2 3 2 2 3 2 2 3 2 3 3 2 5 4 3 3 3 2 2 4 2 1 2 2\n",
      " 4 2 2 2 2 3 2 5 4 4 4 3 3 2 4 1 4 1 4 0 2 4 4 0 1 2 2 2 4 3 2 3 0 3 2 4 3\n",
      " 3 4 2 5 2 4 3 2 2 3 2 3 2 3 2 4 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "X_train=X_norm2\n",
    "y_train=y_train2\n",
    "n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval\n",
    "print(int(len(y_train)*0.8)) ## Let's use 80% to train and 20% to eval\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(y_train)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.71893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-mlogloss:1.66690\n",
      "[2]\tvalidation_0-mlogloss:1.62169\n",
      "[3]\tvalidation_0-mlogloss:1.58585\n",
      "[4]\tvalidation_0-mlogloss:1.55297\n",
      "[5]\tvalidation_0-mlogloss:1.52303\n",
      "[6]\tvalidation_0-mlogloss:1.50433\n",
      "[7]\tvalidation_0-mlogloss:1.48916\n",
      "[8]\tvalidation_0-mlogloss:1.47651\n",
      "[9]\tvalidation_0-mlogloss:1.46262\n",
      "[10]\tvalidation_0-mlogloss:1.45209\n",
      "[11]\tvalidation_0-mlogloss:1.44462\n",
      "[12]\tvalidation_0-mlogloss:1.43704\n",
      "[13]\tvalidation_0-mlogloss:1.43036\n",
      "[14]\tvalidation_0-mlogloss:1.42855\n",
      "[15]\tvalidation_0-mlogloss:1.42244\n",
      "[16]\tvalidation_0-mlogloss:1.41865\n",
      "[17]\tvalidation_0-mlogloss:1.41658\n",
      "[18]\tvalidation_0-mlogloss:1.41635\n",
      "[19]\tvalidation_0-mlogloss:1.41613\n",
      "[20]\tvalidation_0-mlogloss:1.41336\n",
      "[21]\tvalidation_0-mlogloss:1.41384\n",
      "[22]\tvalidation_0-mlogloss:1.41554\n",
      "[23]\tvalidation_0-mlogloss:1.41647\n",
      "[24]\tvalidation_0-mlogloss:1.42177\n",
      "[25]\tvalidation_0-mlogloss:1.42716\n",
      "[26]\tvalidation_0-mlogloss:1.43555\n",
      "[27]\tvalidation_0-mlogloss:1.44156\n",
      "[28]\tvalidation_0-mlogloss:1.44729\n",
      "[29]\tvalidation_0-mlogloss:1.45357\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8537\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8537\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
