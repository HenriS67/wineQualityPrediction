{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: LES ARBRES DE DECISION: \n",
    "Nous allons tester maintenant les arbres de décisions. Dans ces arbres chaque noeud divise les observations en 2 branches en fonction d'une condition (par exemple alcohol<=12) et chaque feuille de l'arbre correspondra à une qualité égale à la moyenne de la qualité des observations présentes dans la feuille.\n",
    "Une nouvelle observation x n'aura qu'à suivre le chemin de l'arbre en fonction des conditions des noeuds pour atterir dans une feuille. La prédiction sera alors la qualité de la feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 10)\n",
      "(1074,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hists(df):\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    for n in range(12):\n",
    "        i = n % 3\n",
    "        j = n % 4\n",
    "        ax[i, j].hist(df.iloc[:, n], bins='auto')\n",
    "        ax[i, j].set_xlabel(df.columns[n])\n",
    "\n",
    "#On normalise : mettre entre 0 et 1\n",
    "def normalize(df, property, parameter):\n",
    "    df[property] = np.log(df[property] + parameter)\n",
    "\n",
    "\n",
    "normalize(df, \"fixed acidity\", -2.3)\n",
    "normalize(df, \"sulphates\", -0.24)\n",
    "normalize(df, \"total sulfur dioxide\", 5)\n",
    "normalize(df, \"residual sugar\", -1.1)\n",
    "normalize(df, \"chlorides\", -0.005)\n",
    "normalize(df, \"volatile acidity\", 2)\n",
    "normalize(df, \"free sulfur dioxide\", 2)\n",
    "#plot_hists(df)\n",
    "\n",
    "standardized = (df - df.mean()) / df.std()\n",
    "standardized = standardized[(np.abs(standardized) < 3).all(axis=1)]\n",
    "rows = np.setdiff1d(list(df.index), list(standardized.index))\n",
    "df.drop(index=rows, inplace=True)\n",
    "#plot_hists(df)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Préparation des données\n",
    "y = df['quality']\n",
    "X= [df['fixed acidity'],  df['volatile acidity']  ,df['citric acid']  ,df['residual sugar'],  df['chlorides'],df['free sulfur dioxide']  ,df['total sulfur dioxide'],  df['density']    ,df['pH'],  df['sulphates']]\n",
    "X=np.transpose(np.array(X))\n",
    "y=np.asarray(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_features = ['fixed acidity',  'volatile acidity'  ,'citric acid'  ,'residual sugar',  'chlorides','free sulfur dioxide'  ,'total sulfur dioxide',  'density'    ,'pH',  'sulphates']\n",
    "nb_feature=len(X_features)\n",
    "\n",
    "\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 2)\n",
    "supp=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]==5 or y[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.5):\n",
    "            supp.append(i)\n",
    "y2=np.delete(y,supp)\n",
    "\n",
    "X2=np.delete(X,supp,0)\n",
    "\n",
    "#Plot des modifications\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(y, bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality avant modification\")\n",
    "\n",
    "ax[1].hist(y2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality après modification\")\n",
    "\n",
    "#on créé les jeux de données\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X2, y2, test_size=0.4, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    " # normalize the original features\n",
    "X_train, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "X_cv, X_mu, X_sigma = zscore_normalize_features(X_cv)\n",
    "X_test, X_mu, X_sigma = zscore_normalize_features(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencer par mettre en place un arbre de décision:\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "- on commence à la racine avec tout le dataset qu'on veut split\n",
    "- on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12)\n",
    "- on décide le split choisi en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split\n",
    "- on sépare le dataset en deux et on refait récursivement la même chose sur les 2 nouveaux noeuds.\n",
    "- on s'arrête lorsque un noeud est totalement pure (=tous les vins de meme qualité), ou à une certaine profondeur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code de l'arbre pour une multi-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de l'impureté d'un noeuf, pour savoir à quel point le noeud est pur (=les vins qui s'y trouvent ont la même qualité)\n",
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "#split du noeud pour des valeurs continues (ex:split en fonction de la condition {X_alcohol<=12.355?})\n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i in node_indices:\n",
    "        if X[i,feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "#calcul du gain d'information = utilité d'un split, permet de choisir sur quelle condition on va split le noeud\n",
    "def compute_information_gain_continue(X, y, node_indices, feature, t):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,t)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = gini_Impurity(y_node)\n",
    "    left_entropy = gini_Impurity(y_left)\n",
    "    right_entropy = gini_Impurity(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "#garder la meilleur condition pour le meilleur split\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    tmax=0\n",
    "\n",
    "    tab_max_feature=np.zeros(num_features)\n",
    "    tab_min_feature=np.zeros(num_features)\n",
    "    for loop in range(num_features):\n",
    "        tab_max_feature[loop]=np.max(np.transpose(X)[loop])\n",
    "        tab_min_feature[loop]=np.min(np.transpose(X)[loop])\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        tab_t_feature=np.linspace(tab_min_feature[feature], tab_max_feature[feature], len(X)-1)\n",
    "        \n",
    "        for t in range(len(tab_t_feature)):\n",
    "            info_gain = compute_information_gain_continue(X, y, node_indices, feature,tab_t_feature[t])\n",
    "\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                tmax=tab_t_feature[t]\n",
    "   \n",
    "    return best_feature,tmax,max_info_gain\n",
    "\n",
    "#construction recursive de l'arbre de décision:\n",
    "#on commence à la racine avec tout le dataset\n",
    "#on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12)\n",
    "#on décide la condition choisie en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split\n",
    "#on sépare le dataset en deux et on refait récursivement la même chose sur les 2 nouveaux noeuds.\n",
    "#on s'arrête lorsque un noeud est totalement pure (=tous les vins de meme qualité), ou à une certaine profondeur\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        print(formatting,\"note moyenne attribuée à la feuille :\",np.mean(y[node_indices]),\"(\",round(np.mean(y[node_indices])),\")\")\n",
    "        return 0\n",
    "   \n",
    "\n",
    "    best_feature,tmax,max_info = get_best_split_continue(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %s <= %s, pour un gain de %s\" % (formatting, current_depth, branch_name, X_features[best_feature], tmax,max_info))\n",
    "\n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, best_feature,tmax)\n",
    "    tree.append((left_indices, right_indices, best_feature,tmax))\n",
    "    \n",
    "    build_tree_recursive_continue(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive_continue(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de l'arbre de décision: On ne va qu'à une profondeur de 2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42683/3297919408.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  impurity-=(tab_value[loop]/sum(tab_value))**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: sulphates <= -0.4387070165807345, pour un gain de 0.05362897506253106\n",
      "- Depth 1, Left: Split on feature: density <= 0.12832861014549923, pour un gain de 0.06507649925300424\n",
      "-- Depth 2, Left: Split on feature: sulphates <= -1.0524893322866307, pour un gain de 0.03761937716262964\n",
      "   --- Left leaf node with indices [6, 17, 23, 26, 29, 38, 42, 66, 68, 78, 92, 99, 111, 120, 128, 176, 179, 192, 213, 224, 232, 247, 268, 277, 279, 280, 287, 295, 320, 331, 346, 347, 351, 366, 372]\n",
      "   --- note moyenne attribuée à la feuille : 5.2 ( 5 )\n",
      "   --- Right leaf node with indices [2, 3, 8, 30, 31, 33, 41, 45, 51, 57, 59, 73, 75, 79, 95, 109, 117, 124, 125, 126, 142, 146, 154, 167, 172, 186, 191, 194, 201, 210, 215, 217, 219, 223, 225, 226, 231, 234, 238, 246, 272, 291, 293, 299, 313, 328, 338, 343, 348, 370]\n",
      "   --- note moyenne attribuée à la feuille : 5.5 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: residual sugar <= 0.4592063818332184, pour un gain de 0.020710059171597767\n",
      "   --- Left leaf node with indices [10, 19, 28, 32, 44, 54, 61, 71, 86, 90, 114, 119, 127, 139, 157, 170, 182, 184, 206, 212, 229, 241, 243, 248, 251, 253, 254, 255, 260, 267, 271, 274, 286, 289, 301, 325, 332, 354, 364]\n",
      "   --- note moyenne attribuée à la feuille : 5.0 ( 5 )\n",
      "   --- Right leaf node with indices [108, 129, 151, 152, 262, 278, 281, 292, 298, 306, 330, 349, 358]\n",
      "   --- note moyenne attribuée à la feuille : 5.153846153846154 ( 5 )\n",
      "- Depth 1, Right: Split on feature: sulphates <= 0.6507565937972317, pour un gain de 0.03885101219877507\n",
      "-- Depth 2, Left: Split on feature: density <= -0.1577325478947853, pour un gain de 0.03980828932095992\n",
      "   --- Left leaf node with indices [0, 1, 5, 12, 14, 25, 37, 49, 58, 67, 72, 77, 91, 94, 96, 97, 100, 106, 116, 118, 132, 137, 138, 140, 147, 160, 161, 162, 164, 166, 171, 181, 183, 190, 200, 205, 220, 228, 249, 258, 270, 284, 290, 303, 309, 311, 315, 318, 321, 322, 327, 336, 337, 342, 363, 368, 373]\n",
      "   --- note moyenne attribuée à la feuille : 6.12280701754386 ( 6 )\n",
      "   --- Right leaf node with indices [4, 13, 18, 20, 34, 35, 46, 48, 50, 60, 62, 64, 65, 69, 70, 76, 81, 87, 88, 105, 112, 115, 121, 122, 123, 130, 133, 134, 135, 136, 144, 145, 150, 153, 158, 159, 163, 168, 169, 174, 177, 185, 188, 196, 199, 211, 214, 216, 227, 233, 235, 236, 239, 242, 244, 250, 252, 259, 261, 264, 265, 266, 273, 296, 300, 302, 308, 310, 312, 314, 339, 344, 352, 357, 359, 361, 369, 371]\n",
      "   --- note moyenne attribuée à la feuille : 5.615384615384615 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: density <= -0.4914705656084508, pour un gain de 0.07956467002949985\n",
      "   --- Left leaf node with indices [7, 16, 27, 52, 53, 55, 63, 80, 84, 89, 104, 149, 156, 175, 193, 198, 203, 245, 257, 263, 282, 283, 323, 333, 334, 335, 345, 350, 356]\n",
      "   --- note moyenne attribuée à la feuille : 6.931034482758621 ( 7 )\n",
      "   --- Right leaf node with indices [9, 11, 15, 21, 22, 24, 36, 39, 40, 43, 47, 56, 74, 82, 83, 85, 93, 98, 101, 102, 103, 107, 110, 113, 131, 141, 143, 148, 155, 165, 173, 178, 180, 187, 189, 195, 197, 202, 204, 207, 208, 209, 218, 221, 222, 230, 237, 240, 256, 269, 275, 276, 285, 288, 294, 297, 304, 305, 307, 316, 317, 319, 324, 326, 329, 340, 341, 353, 355, 360, 362, 365, 367, 374]\n",
      "   --- note moyenne attribuée à la feuille : 6.175675675675675 ( 6 )\n"
     ]
    }
   ],
   "source": [
    "tree = []\n",
    "root_indices=list(range(0, len(X_train)))\n",
    "build_tree_recursive_continue(X_train, y_train,root_indices, \"Root\", max_depth=3, current_depth=0, tree = tree);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Il faudrait faire passer toutes les observations du jeu de test dans l arbre pour obtenir leurs notes prédites. \n",
    "Mais nous allons directement mettre en place la random forest à l aide la bibliothèque sklearn et xgboost.\n",
    "Random forest va créer plein d arbres de décision avec une part d aléatoire dans le choix des splits et choisir l arbre final graĉe à un système de votes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST POUR LA QUALITE DU VIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test=le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.76413\n",
      "[1]\tvalidation_0-mlogloss:1.74212\n",
      "[2]\tvalidation_0-mlogloss:1.74340\n",
      "[3]\tvalidation_0-mlogloss:1.72256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-mlogloss:1.72616\n",
      "[5]\tvalidation_0-mlogloss:1.69494\n",
      "[6]\tvalidation_0-mlogloss:1.69315\n",
      "[7]\tvalidation_0-mlogloss:1.69298\n",
      "[8]\tvalidation_0-mlogloss:1.70397\n",
      "[9]\tvalidation_0-mlogloss:1.72146\n",
      "[10]\tvalidation_0-mlogloss:1.72980\n",
      "[11]\tvalidation_0-mlogloss:1.74967\n",
      "[12]\tvalidation_0-mlogloss:1.76279\n",
      "[13]\tvalidation_0-mlogloss:1.77983\n",
      "[14]\tvalidation_0-mlogloss:1.78464\n",
      "[15]\tvalidation_0-mlogloss:1.79472\n",
      "[16]\tvalidation_0-mlogloss:1.79150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train,y_train, eval_set = [(X_test,y_test)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8093\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8093\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle random Forest nous permet d'obtenir sur le jeu de test 80% de précision, c'est le meilleur score obtenu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
