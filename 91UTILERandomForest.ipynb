{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: LES ARBRES DE DECISION: \n",
    "Nous allons tester maintenant les arbres de décisions. Dans ces arbres chaque noeud divise les observations en 2 branches en fonction d'une condition (par exemple alcohol<=12) et chaque feuille de l'arbre correspondra à une qualité égale à la moyenne de la qualité des observations présentes dans la feuille.\n",
    "Une nouvelle observation x n'aura qu'à suivre le chemin de l'arbre en fonction des conditions des noeuds pour atterir dans une feuille. La prédiction sera alors la qualité de la feuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 10)\n",
      "(1074,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hists(df):\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    for n in range(12):\n",
    "        i = n % 3\n",
    "        j = n % 4\n",
    "        ax[i, j].hist(df.iloc[:, n], bins='auto')\n",
    "        ax[i, j].set_xlabel(df.columns[n])\n",
    "\n",
    "#On normalise : mettre entre 0 et 1\n",
    "def normalize(df, property, parameter):\n",
    "    df[property] = np.log(df[property] + parameter)\n",
    "\n",
    "\n",
    "normalize(df, \"fixed acidity\", -2.3)\n",
    "normalize(df, \"sulphates\", -0.24)\n",
    "normalize(df, \"total sulfur dioxide\", 5)\n",
    "normalize(df, \"residual sugar\", -1.1)\n",
    "normalize(df, \"chlorides\", -0.005)\n",
    "normalize(df, \"volatile acidity\", 2)\n",
    "normalize(df, \"free sulfur dioxide\", 2)\n",
    "#plot_hists(df)\n",
    "\n",
    "standardized = (df - df.mean()) / df.std()\n",
    "standardized = standardized[(np.abs(standardized) < 3).all(axis=1)]\n",
    "rows = np.setdiff1d(list(df.index), list(standardized.index))\n",
    "df.drop(index=rows, inplace=True)\n",
    "#plot_hists(df)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Préparation des données\n",
    "y = df['quality']\n",
    "X= [df['fixed acidity'],  df['volatile acidity']  ,df['citric acid']  ,df['residual sugar'],  df['chlorides'],df['free sulfur dioxide']  ,df['total sulfur dioxide'],  df['density']    ,df['pH'],  df['sulphates']]\n",
    "X=np.transpose(np.array(X))\n",
    "y=np.asarray(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_features = ['fixed acidity',  'volatile acidity'  ,'citric acid'  ,'residual sugar',  'chlorides','free sulfur dioxide'  ,'total sulfur dioxide',  'density'    ,'pH',  'sulphates']\n",
    "nb_feature=len(X_features)\n",
    "\n",
    "\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 2)\n",
    "supp=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]==5 or y[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.5):\n",
    "            supp.append(i)\n",
    "y2=np.delete(y,supp)\n",
    "\n",
    "X2=np.delete(X,supp,0)\n",
    "\n",
    "#Plot des modifications\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(y, bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality avant modification\")\n",
    "\n",
    "ax[1].hist(y2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality après modification\")\n",
    "\n",
    "#on créé les jeux de données\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X2, y2, test_size=0.4, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    " # normalize the original features\n",
    "X_train, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "X_cv, X_mu, X_sigma = zscore_normalize_features(X_cv)\n",
    "X_test, X_mu, X_sigma = zscore_normalize_features(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencer par mettre en place un arbre de décision:\n",
    "\n",
    "Pour cela :\n",
    "\n",
    "- on commence à la racine avec tout le dataset qu'on veut split\n",
    "- on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12)\n",
    "- on décide le split choisi en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split\n",
    "- on sépare le dataset en deux et on refait récursivement la même chose sur les 2 nouveaux noeuds.\n",
    "- on s'arrête lorsque un noeud est totalement pure (=tous les vins de meme qualité), ou à une certaine profondeur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code de l'arbre pour la prédiction du vin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbreBinaireVin:\n",
    "    def __init__(self):\n",
    "        self.qualite = 0\n",
    "        self.split=0\n",
    "        self.carac=0\n",
    "        self.enfant_gauche = None\n",
    "        self.enfant_droit = None\n",
    "\n",
    "\n",
    "    def insert_gauche(self):\n",
    "        self.enfant_gauche = ArbreBinaireVin()\n",
    "\n",
    "    def insert_droit(self):\n",
    "        self.enfant_droit = ArbreBinaireVin()\n",
    "\n",
    "    def get_valeur(self):\n",
    "        return self.valeur\n",
    "\n",
    "    def get_gauche(self):\n",
    "        return self.enfant_gauche\n",
    "\n",
    "    def get_droit(self):\n",
    "        return self.enfant_droit\n",
    "\n",
    "    \n",
    "    def get_predictionVin(self,x):\n",
    "\n",
    "        if(x[self.carac]<=self.split):\n",
    "            if(self.enfant_gauche==None):\n",
    "                return self.qualite\n",
    "            else:\n",
    "                return self.enfant_gauche.get_predictionVin(x)\n",
    "        else:\n",
    "            if(self.enfant_droit==None):\n",
    "                return self.qualite\n",
    "            else:\n",
    "                return self.enfant_droit.get_predictionVin(x)\n",
    "\n",
    "    def affiche(self):\n",
    "        print(self.carac,X_features[self.carac],self.split,self.qualite)\n",
    "        if(self.enfant_gauche!=None):\n",
    "            self.enfant_gauche.affiche()\n",
    "        if(self.enfant_droit!=None):\n",
    "            self.enfant_droit.affiche()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code de la construction de l'arbre avec le dataset d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de l'impureté d'un noeuf, pour savoir à quel point le noeud est pur (=les vins qui s'y trouvent ont la même qualité)\n",
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "#split du noeud pour des valeurs continues (ex:split en fonction de la condition {X_alcohol<=12.355?})\n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i in node_indices:\n",
    "        if X[i,feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "#calcul du gain d'information = utilité d'un split, permet de choisir sur quelle condition on va split le noeud\n",
    "def compute_information_gain_continue(X, y, node_indices, feature, t):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,t)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = gini_Impurity(y_node)\n",
    "    left_entropy = gini_Impurity(y_left)\n",
    "    right_entropy = gini_Impurity(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "#garder la meilleur condition pour le meilleur split\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    tmax=0\n",
    "\n",
    "    tab_max_feature=np.zeros(num_features)\n",
    "    tab_min_feature=np.zeros(num_features)\n",
    "    for loop in range(num_features):\n",
    "        tab_max_feature[loop]=np.max(np.transpose(X)[loop])\n",
    "        tab_min_feature[loop]=np.min(np.transpose(X)[loop])\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        tab_t_feature=np.linspace(tab_min_feature[feature], tab_max_feature[feature], len(X)-1)\n",
    "        \n",
    "        for t in range(len(tab_t_feature)):\n",
    "            info_gain = compute_information_gain_continue(X, y, node_indices, feature,tab_t_feature[t])\n",
    "\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                tmax=tab_t_feature[t]\n",
    "   \n",
    "    return best_feature,tmax,max_info_gain\n",
    "\n",
    "#construction recursive de l'arbre de décision:\n",
    "#on commence à la racine avec tout le dataset\n",
    "#on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12)\n",
    "#on décide la condition choisie en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split\n",
    "#on sépare le dataset en deux et on refait récursivement la même chose sur les 2 nouveaux noeuds.\n",
    "#on s'arrête lorsque un noeud est totalement pure (=tous les vins de meme qualité), ou à une certaine profondeur\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree, arbreVin):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        qualite_node=np.mean(y[node_indices])\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        print(formatting,\"note moyenne attribuée à la feuille :\",qualite_node,\"(\",round(np.mean(y[node_indices])),\")\")\n",
    "\n",
    "        arbreVin.qualite=round(qualite_node)\n",
    "\n",
    "        return 0\n",
    "   \n",
    "    \n",
    "    best_feature,tmax,max_info = get_best_split_continue(X, y, node_indices) \n",
    "    arbreVin.carac=best_feature\n",
    "    arbreVin.split=tmax\n",
    "\n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %s <= %s, pour un gain de %s\" % (formatting, current_depth, branch_name, X_features[best_feature], tmax,max_info))\n",
    "\n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, best_feature,tmax)\n",
    "    tree.append((left_indices, right_indices, best_feature,tmax))\n",
    "    \n",
    "    if(len(left_indices)>1):\n",
    "        arbreVin.insert_gauche()\n",
    "        build_tree_recursive_continue(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree,arbreVin.enfant_gauche)\n",
    "    if(len(right_indices)>1):\n",
    "        arbreVin.insert_droit()\n",
    "        build_tree_recursive_continue(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree,arbreVin.enfant_droit)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de l'arbre de décision avec une profondeur max de 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42683/506145069.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  impurity-=(tab_value[loop]/sum(tab_value))**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: sulphates <= -0.09272595611982037, pour un gain de 0.052469323042744875\n",
      "- Depth 1, Left: Split on feature: total sulfur dioxide <= 0.853198588681813, pour un gain de 0.04485148373737924\n",
      "-- Depth 2, Left: Split on feature: density <= 0.41867845114657687, pour un gain de 0.04332317923273732\n",
      "--- Depth 3, Left: Split on feature: residual sugar <= 1.1300734780067812, pour un gain de 0.05027147181483804\n",
      "    ---- Left leaf node with indices [2, 6, 13, 16, 17, 27, 33, 35, 37, 39, 43, 44, 46, 49, 56, 59, 63, 64, 70, 71, 73, 74, 76, 78, 79, 88, 93, 97, 103, 105, 110, 113, 118, 119, 120, 121, 125, 130, 136, 143, 150, 163, 164, 169, 173, 177, 179, 182, 183, 187, 188, 191, 194, 200, 209, 210, 215, 216, 219, 222, 225, 226, 228, 235, 236, 241, 243, 244, 248, 252, 255, 256, 260, 264, 266, 271, 272, 281, 283, 284, 286, 290, 291, 295, 306, 311, 314, 316, 320, 332, 335, 337, 344, 345, 355, 356, 365, 369, 374]\n",
      "    ---- note moyenne attribuée à la feuille : 5.636363636363637 ( 6 )\n",
      "    ---- Right leaf node with indices [42, 81, 92, 192, 279, 310, 324, 351]\n",
      "    ---- note moyenne attribuée à la feuille : 5.875 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: total sulfur dioxide <= 0.5224497635627086, pour un gain de 0.052743342516069824\n",
      "    ---- Left leaf node with indices [11, 22, 26, 29, 30, 55, 67, 91, 109, 111, 114, 124, 148, 158, 174, 178, 186, 190, 201, 229, 233, 238, 242, 245, 246, 247, 251, 262, 267, 285, 298, 307, 318, 339, 348, 366]\n",
      "    ---- note moyenne attribuée à la feuille : 5.166666666666667 ( 5 )\n",
      "    ---- Right leaf node with indices [53, 112, 153, 185, 224, 278, 349, 353]\n",
      "    ---- note moyenne attribuée à la feuille : 5.625 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: fixed acidity <= -2.8656468717590577, pour un gain de 0.036946019743751335\n",
      "--- Depth 3, Right: Split on feature: free sulfur dioxide <= 0.6597548370368371, pour un gain de 0.03752486629679613\n",
      "    ---- Left leaf node with indices [4, 5, 9, 57, 68, 80, 87, 123, 128, 217, 232, 240, 249, 265, 277, 303, 338, 357, 367]\n",
      "    ---- note moyenne attribuée à la feuille : 5.315789473684211 ( 5 )\n",
      "    ---- Right leaf node with indices [12, 47, 62, 65, 69, 104, 106, 116, 134, 137, 140, 146, 152, 154, 168, 176, 193, 206, 214, 254, 287, 299, 301, 333, 336, 362]\n",
      "    ---- note moyenne attribuée à la feuille : 5.038461538461538 ( 5 )\n",
      "- Depth 1, Right: Split on feature: citric acid <= 0.15676519921971788, pour un gain de 0.061485402736593864\n",
      "-- Depth 2, Left: Split on feature: pH <= 0.3387519339878331, pour un gain de 0.05154936838208535\n",
      "--- Depth 3, Left: Split on feature: density <= 0.6547929676033211, pour un gain de 0.1570550931430834\n",
      "    ---- Left leaf node with indices [0, 19, 32, 51, 52, 58, 61, 66, 115, 122, 138, 159, 160, 184, 258, 261, 276, 315, 317, 325, 327, 329, 330, 375]\n",
      "    ---- note moyenne attribuée à la feuille : 5.625 ( 6 )\n",
      "    ---- Right leaf node with indices [25, 86, 94, 142, 151]\n",
      "    ---- note moyenne attribuée à la feuille : 6.0 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: fixed acidity <= -2.350583798240976, pour un gain de 0.1004821389436773\n",
      "    ---- Left leaf node with indices [135, 141, 308]\n",
      "    ---- note moyenne attribuée à la feuille : 7.0 ( 7 )\n",
      "    ---- Right leaf node with indices [10, 20, 21, 24, 40, 45, 101, 102, 107, 132, 133, 144, 157, 165, 172, 203, 208, 213, 218, 220, 223, 227, 250, 274, 288, 289, 300, 309, 321, 334, 340, 347, 350, 364, 372, 373]\n",
      "    ---- note moyenne attribuée à la feuille : 5.75 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: total sulfur dioxide <= 0.7429489803087783, pour un gain de 0.027252765090602815\n",
      "--- Depth 3, Left: Split on feature: density <= 0.4029374833827939, pour un gain de 0.03418658363713323\n",
      "    ---- Left leaf node with indices [1, 3, 7, 8, 15, 18, 23, 28, 34, 38, 48, 50, 54, 60, 75, 77, 83, 85, 89, 98, 99, 100, 117, 126, 129, 147, 155, 161, 162, 166, 180, 199, 202, 204, 207, 230, 231, 253, 263, 268, 270, 280, 296, 304, 322, 323, 326, 328, 341, 342, 343, 346, 370, 371]\n",
      "    ---- note moyenne attribuée à la feuille : 6.888888888888889 ( 7 )\n",
      "    ---- Right leaf node with indices [14, 36, 41, 72, 96, 127, 145, 149, 156, 171, 175, 181, 196, 197, 198, 211, 212, 221, 234, 237, 257, 259, 269, 275, 293, 294, 302, 312, 313, 319, 352, 354, 359, 360, 361, 363, 368]\n",
      "    ---- note moyenne attribuée à la feuille : 6.486486486486487 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: total sulfur dioxide <= 1.245197196230381, pour un gain de 0.13250000000000017\n",
      "    ---- Left leaf node with indices [82, 84, 90, 95, 139, 167, 189, 195, 205, 282, 305, 331]\n",
      "    ---- note moyenne attribuée à la feuille : 5.916666666666667 ( 6 )\n",
      "    ---- Right leaf node with indices [31, 108, 131, 170, 239, 292, 297, 358]\n",
      "    ---- note moyenne attribuée à la feuille : 6.25 ( 6 )\n"
     ]
    }
   ],
   "source": [
    "tree = []\n",
    "arbre = ArbreBinaireVin()\n",
    "root_indices=list(range(0, len(X_train)))\n",
    "build_tree_recursive_continue(X_train, y_train,root_indices, \"Root\", max_depth=4, current_depth=0, tree = tree, arbreVin=arbre);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecteur x [ 1.65989152 -0.56212655  1.29038469 -1.73235125  0.93061168  0.88161709\n",
      "  0.30550152  0.39591505 -1.94880838  0.87331486]\n",
      "note prédite: 7\n",
      "vraie note : 7\n"
     ]
    }
   ],
   "source": [
    "x=X_train[50]\n",
    "print(\"vecteur x\",x)\n",
    "print(\"note prédite:\",arbre.get_predictionVin(x))\n",
    "print(\"vraie note :\",y_train[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On a construit l'arbre de profondeur max 4, il ne reste plus qu'à faire passer notre jeu de test dedans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_error(yhat,y_test):\n",
    "   \n",
    "    cost = 0.0\n",
    "    m=len(y_test)\n",
    "    for i in range(m):                                \n",
    "        cost = cost + (y_test[i] - yhat[i])**2   #scalar\n",
    "    cost = cost / (2 * m)                 #scalar    \n",
    "    return cost\n",
    "\n",
    "def predict_model(arbre,X,y):\n",
    "    yhat=[]\n",
    "    for loop in range(len(X)):\n",
    "        yhat.append(arbre.get_predictionVin(X[loop]))\n",
    "\n",
    "    sum=0\n",
    "    for loop in range(len(yhat)):\n",
    "        if(y[loop] == yhat[loop]):\n",
    "            sum+=1\n",
    "    print('Train Accuracy (%): ',(sum/len(yhat))*100)\n",
    "\n",
    "    nn_test_error =calcul_error(yhat,y)\n",
    "\n",
    "    print(f\"Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (%):  53.96825396825397\n",
      "Set Classification Error: 0.3135\n"
     ]
    }
   ],
   "source": [
    "predict_model(arbre,X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons 53% de bonnes prédictions sur le jeu de test.\n",
    "\n",
    "Nous allons enfin mettre en place la random forest à l aide la bibliothèque sklearn et xgboost.\n",
    "Random forest va créer plein d arbres de décision de ce type avec une part d aléatoire dans le choix des splits et choisir l arbre final grâce à un système de votes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST POUR LA QUALITE DU VIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test=le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.76413\n",
      "[1]\tvalidation_0-mlogloss:1.74212\n",
      "[2]\tvalidation_0-mlogloss:1.74340\n",
      "[3]\tvalidation_0-mlogloss:1.72256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-mlogloss:1.72616\n",
      "[5]\tvalidation_0-mlogloss:1.69494\n",
      "[6]\tvalidation_0-mlogloss:1.69315\n",
      "[7]\tvalidation_0-mlogloss:1.69298\n",
      "[8]\tvalidation_0-mlogloss:1.70397\n",
      "[9]\tvalidation_0-mlogloss:1.72146\n",
      "[10]\tvalidation_0-mlogloss:1.72980\n",
      "[11]\tvalidation_0-mlogloss:1.74967\n",
      "[12]\tvalidation_0-mlogloss:1.76279\n",
      "[13]\tvalidation_0-mlogloss:1.77983\n",
      "[14]\tvalidation_0-mlogloss:1.78464\n",
      "[15]\tvalidation_0-mlogloss:1.79472\n",
      "[16]\tvalidation_0-mlogloss:1.79150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train,y_train, eval_set = [(X_test,y_test)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8093\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8093\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle random Forest nous permet d'obtenir sur le jeu de test 80% de précision, c'est le meilleur score obtenu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
