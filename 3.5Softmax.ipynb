{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 1.94181282 -0.59360107  0.1308811  -1.36502663]\n",
      " [ 1.27349242 -0.59360107 -0.04525363 -1.16156762]\n",
      " ...\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "[5 7 7 5 5 7 5 6 6 7 4 5 4 6 6 5 5 6 5 5 5 6 5 4 5 6 4 5 5 6 6 5 4 5 6 6 5\n",
      " 5 5 5 5 5 6 5 5 7 5 5 5 4 5 5 5 4 6 5 5 4 4 6 5 6 5 5 5 5 5 6 6 4 7 7 7 5\n",
      " 6 6 6 7 6 6 7 7 6 6 5 5 5 5 7 5 4 5 5 4 8 6 6 8 7 7 7 5 6 7 7 5 5 6 7 7 6\n",
      " 5 7 7 6 7 6 6 5 6 7 7 5 5 7 7 7 6 6 7 6 6 8 6 5 7 6 7 4 7 5 6 5 5 7 5 7 7\n",
      " 6 6 7 5 6 5 8 7 7 5 5 6 6 6 7 8 3 6 5 6 5 6 5 8 5 6 7 7 6 8 6 8 6 7 7 7 7\n",
      " 7 7 6 6 7 7 5 3 6 5 6 5 6 7 5 6 6 6 6 6 6 6 5 5 5 5 6 4 5 5 6 7 8 6 5 5 5\n",
      " 6 6 6 5 6 5 6 5 5 5 5 4 5 5 5 4 7 6 5 7 5 6 5 5 5 5 5 6 6 4 4 5 5 5 5 5 5\n",
      " 5 6 5 5 5 5 5 6 5 5 6 5 6 6 5 6 6 5 6 5 5 5 6 6 5 7 7 6 7 5 6 5 4 7 5 7 4\n",
      " 6 4 7 7 7 5 5 5 6 7 7 4 7 6 5 6 7 6 7 7 7 7 6 6 6 5 6 6 7 4 7 6 5 7 7 7 7\n",
      " 7 7 7 7 7 7 7 6 5 5 7 7 6 6 5 6 6 7 5 7 7 7 7 7 6 5 7 6 7 7 5 6 7 7 7 6 6\n",
      " 6 6 6 6 7 7 5 7 7 6 8 6 6 7 7 7 7 7 5 7 6 7 7 8 7 6 6 6 7 5 7 6 8 7 6 5 7\n",
      " 7 7 6 6 6 6 6 5 7 6 6 5 7 7 7 5 7 6 6 6 4 4 7 6 6 6 6 6 7 8 5 7 7 7 7 6 6\n",
      " 6 6 6 7 5 5 6 4 4 5 6 5 4 6 6 6 6 4 6 7 6 6 5 5 5 3 5 5 6 6 6 6 6 5 5 5 6\n",
      " 6 6 6 6 5 6 5 6 6 5 5 5 5 5 5 6 6 5 8 6 6 7 6 5 7 5 5 5 6 4 5 5 7 6 5 5 6\n",
      " 5 5 8 7 7 7 6 7 6 4 7 4 7 3 5 7 7 3 4 5 5 5 5 5 7 6 5 6 6 3 6 5 6 6 6 5 6\n",
      " 7 5 6 7 6 5 8 7 6 5 5 5 5 6 5 5 6 6 6 6 7 6 6 6 5 5]\n",
      "[[ 0.71655875 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 0.66086539 -0.4087107  -1.10206203 -1.36502663]\n",
      " [ 0.27101182 -0.87093663 -0.51494625 -1.26329712]\n",
      " ...\n",
      " [-0.11884175  0.51574115  0.54186214 -0.70378484]\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAATW0lEQVR4nO3dfZBddX3H8fe3LMiTJQLbFJLI4khVbCvQyEN11IJteaqhFilWMWCc9AHUqjMarK2PVZxxBJx2cBhQgspTEQoFqyIPolWoARSBVI0YTCKQBULkQYTIt3+c3+rNsrv37ubu3ru/vF8zO3vO75x7zvdkv/ns2bPnno3MRJJUl9/qdQGSpO4z3CWpQoa7JFXIcJekChnuklQhw12SKmS4TyAihiIiI2KgzP93RCzu0rbPi4iPTPG1O0TEf0XExoj4j27U0ysRsToiXl2m3xsR57Qs+8uIWBMRj0bE/r2rcuswnf3ewb5n9de6H/t4YKZ2VIPMPGJkOiJOBN6SmS/vQSnHAnOB3TJzUw/2Py0y86Ojhj4BnJKZV/Sinq3dDPf7r7/WI99kgG1nY3/3Sx975j477QX8cCqNP3JWNkvsBdzZ6yI0I7r2te7DHu9NH2fmrP8A9gduBR4BLgYuAj5Slp0IfHPU+gk8v0wfBdwG/BxYA3ygZb2hsu5Amb8BeAvwIuAJ4FfAo8DDwEuB+4FtWl7/WuB749R8HvBp4JpS99eBvVqWv7Asewj4AXBcGf8g8CTwVNn3Eppv0u8D7gHWA+cDu4w6hiXAT4Eby/ibgZXABuArrfseVefI608q/z4bgL8rx3t7OfZ/a1l/3FrK8hPKsgeBfwJWA68uyz4AfB54Vjm2BB4DftzrHuunj1na72Pud6yvdenTLOOPAoe069my/snAj4Cf2Mc5+8Md2K78I78D2JbmksVTk2j2VwF/UL6Yf1ga9piJmn2C7d4FHNEyfznwrnHqPo/mP+crShOcObI9YKfSgCfRXDrbH3gA2Le1eVq29WZgFfA8YGfgMuBzo47h/LLdHYBFZf0Xle2/D/jWOHWOvP7TwPbAn9H8R/9P4HeAeaX5X9lBLfuWZh855k8Cm0b/pxjr6+THrO/3cfc7Ro2b1VHGJuzZsv41wK7ADvZxVnFZ5mCaJj8jM5/KzEuB73T64sy8ITO/n5lPZ+btwIXAK6dYy3LgjQARsSvw58AFE6x/dWbemJm/pPnuf0hELACOBlZn5mczc1Nm3gZ8EXjdONt5A/DJzLw7Mx8FTgWOH/Xj6Qcy87HM/AXNGcvHMnNlNpd2PgrsFxF7TVDrhzPzicz8Ks1ZyIWZuT4z1wHfoPkG1K6WY4GrWo75n4GnJ9innmlW9nsX9ttJz34sMx8qPT6eraaPawj3PYF1Wb5FFvd0+uKIOCgiro+I4YjYSNNEu0+xls8DfxEROwHHAd/IzHsnWH/NyERpoIdojmcv4KCIeHjkg6bZfnec7ezJ5sd8D83Zzdyx9lW2f2bLth8CgubsZTz3t0z/Yoz5nTuoZU82P+bHaH6sVedmZb93Yb+d9OyasV44ylbTxzWE+73AvIiIlrHntkw/Buw4MhMRowPyAuBKYEFm7kLzY1vQ3jMep1m++3+b5trjCcDn2mxjQUtdO9P8SPkzmsb5embOafnYOTP/fpzt/Iym+Uc8l+bHxNbGba13DfC3o7a/Q2Z+q029nZiolnvZ/Jh3BHbrwj63JrO13yez32fsi856dqzXTdWs7+Mawv3bNP/ob4uIbSPitcCBLcu/B7w4IvaLiO1proe1ejbwUGY+EREHAn/T4X7vB+ZHxHajxs8H3k1zffGyNts4MiJeXrbxYeCmzFwDXAX8XkScUI5p24h4aUS8aJztXAi8IyL2Lt8kPgpcnOPfTfNp4NSIeDFAROwSEeNd8pmsiWq5FDi65Zg/RB09OJNma79PZr/DNJc5ntcyNp09O5ZZ38d9V9BkZeaTNGcOJ9L8qPbXtDRZZv6Q5h//azS/Sf/mqE38A/ChiHgE+Bfgkg53fR3N7U33RcQDLeOX03zHvzwzH2+zjQuA95e6/4hy/TIzH6H5hc/xNGcQ9wEfp/nlzVg+Q3PWdCPwE5pfFL11vJ1m5uVlexdFxM+BO4Ajxlt/ksatJTPvpLmj4QKas58NwNou7XerMIv7veP9lu38K/A/5TLMwdPcs2OZ9X0cm1+6q0NEnAeszcz39Wj/P6b5EfJrvdi/ti72u8Yy68/c+01E/BXNtb/rel2LNN3s9/7Vb+/kmtUi4gaae2BPyMy+uzVK6ib7vb9VeVlGkrZ2XpaRpAp1dFkmIlbTvFX+V8CmzFxY3pF2Mc3belfTPPtkQ7n/9kzgSOBx4MTMvHWi7e++++45NDQ0xUOQJnbLLbc8kJmDvdi3va3pNFFvT+aa+59kZustUMuAazPztIhYVubfQ3N70j7l4yDgrPJ5XENDQ6xYsWISpUidi4iO38HZbfa2ptNEvb0ll2UW0TxbgvL5mJbx87NxEzAnIvbYgv1Ikiap03BP4KsRcUtELC1jc1ueI3Efv3mOyTw2f8bDWsZ4ZklELI2IFRGxYnh4eAqlS/3J3lY/6DTcX56ZB9Bccjk5Il7RurA8xGhSt91k5tmZuTAzFw4O9uRyqDQt7G31g47CvTwgiMxcT/N24wOB+0cut5TP68vq62h5qA4wv4xJkmZI23CPiJ0i4tkj0zTPPLmD5glvi8tqi4GRvw94JfCmaBwMbGzz2FtJUpd1crfMXODy8oTRAeCCzPxyRHwHuCQiltA86/i4sv6XaG6DXEVzK+RJXa9akjShtuGemXcDLxlj/EHgsDHGR/6WoSSpR3yHqiRVyHCXpAr5VMgZNLTs6meMrT7tqB5UIql2nrlLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKuSbmLYSvoFK2rp45i5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkirUcbhHxDYRcVtEXFXm946ImyNiVURcHBHblfFnlflVZfnQNNUuSRrHZM7c3w6sbJn/OHB6Zj4f2AAsKeNLgA1l/PSyniRpBnUU7hExHzgKOKfMB3AocGlZZTlwTJleVOYpyw8r60uSZkinZ+5nAO8Gni7zuwEPZ+amMr8WmFem5wFrAMryjWX9zUTE0ohYERErhoeHp1a91IfsbfWDtuEeEUcD6zPzlm7uODPPzsyFmblwcHCwm5uWesreVj8Y6GCdlwGviYgjge2B3wbOBOZExEA5O58PrCvrrwMWAGsjYgDYBXiw65VLksbV9sw9M0/NzPmZOQQcD1yXmW8ArgeOLastBq4o01eWecry6zIzu1q1JGlCW3Kf+3uAd0bEKppr6ueW8XOB3cr4O4FlW1aiJGmyOrks82uZeQNwQ5m+GzhwjHWeAF7XhdokSVPkO1QlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVahtuEfE9hHxvxHxvYi4MyI+WMb3joibI2JVRFwcEduV8WeV+VVl+dA0H4MkaZROztx/CRyamS8B9gMOj4iDgY8Dp2fm84ENwJKy/hJgQxk/vawnSZpBbcM9G4+W2W3LRwKHApeW8eXAMWV6UZmnLD8sIqJbBUuS2uvomntEbBMR3wXWA9cAPwYezsxNZZW1wLwyPQ9YA1CWbwR2G2ObSyNiRUSsGB4e3qKDkPqJva1+0FG4Z+avMnM/YD5wIPDCLd1xZp6dmQszc+Hg4OCWbk7qG/a2+sGk7pbJzIeB64FDgDkRMVAWzQfWlel1wAKAsnwX4MFuFCtJ6kwnd8sMRsScMr0D8KfASpqQP7asthi4okxfWeYpy6/LzOxizZKkNgbar8IewPKI2Ibmm8ElmXlVRNwFXBQRHwFuA84t658LfC4iVgEPAcdPQ92SpAm0DffMvB3Yf4zxu2muv48efwJ4XVeqkyRNie9QlaQKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklShtuEeEQsi4vqIuCsi7oyIt5fxXSPimoj4Ufn8nDIeEfGpiFgVEbdHxAHTfRCSpM11cua+CXhXZu4LHAycHBH7AsuAazNzH+DaMg9wBLBP+VgKnNX1qiVJE2ob7pl5b2beWqYfAVYC84BFwPKy2nLgmDK9CDg/GzcBcyJij24XLkka36SuuUfEELA/cDMwNzPvLYvuA+aW6XnAmpaXrS1jo7e1NCJWRMSK4eHhydYt9S17W/2g43CPiJ2BLwL/mJk/b12WmQnkZHacmWdn5sLMXDg4ODiZl0p9zd5WP+go3CNiW5pg/0JmXlaG7x+53FI+ry/j64AFLS+fX8YkSTNkoN0KERHAucDKzPxky6IrgcXAaeXzFS3jp0TERcBBwMaWyzeSNGlDy66e9GtWn3bUNFQye7QNd+BlwAnA9yPiu2XsvTShfklELAHuAY4ry74EHAmsAh4HTupmwZKk9tqGe2Z+E4hxFh82xvoJnLyFdUmaJSZ7Vr21n1HPFN+hKkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQp38DVWpY2P9yTX/rJpmg9r+CLdn7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFWobbhHxGciYn1E3NEytmtEXBMRPyqfn1PGIyI+FRGrIuL2iDhgOouXJI2tkzP384DDR40tA67NzH2Aa8s8wBHAPuVjKXBWd8qUJE1G23DPzBuBh0YNLwKWl+nlwDEt4+dn4yZgTkTs0aVaJUkdmuo197mZeW+Zvg+YW6bnAWta1ltbxp4hIpZGxIqIWDE8PDzFMqT+Y2+rH2zxL1QzM4GcwuvOzsyFmblwcHBwS8uQ+oa9rX4w1XC/f+RyS/m8voyvAxa0rDe/jEmSZtBUw/1KYHGZXgxc0TL+pnLXzMHAxpbLN5KkGTLQboWIuBB4FbB7RKwF3g+cBlwSEUuAe4DjyupfAo4EVgGPAydNQ82SpDbahntmvn6cRYeNsW4CJ29pUZKkLeM7VCWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mq0ECvC+gXQ8uufsbY6tOO6kElUveM1dcTsefr4Zm7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqpC3QmrW8bZVzVaTvTUVpt7bnrlLUoUMd0mqkOEuSRWalmvuEXE4cCawDXBOZp421W15fVW18tEAmk5dP3OPiG2AfweOAPYFXh8R+3Z7P5Kk8U3HZZkDgVWZeXdmPglcBCyahv1IksYRmdndDUYcCxyemW8p8ycAB2XmKaPWWwosLbMvAH7Q1UKmbnfggV4XMU221mPbKzMHZ6qQPu3tmr/2UPfxTam3e3afe2aeDZzdq/2PJyJWZObCXtcxHTy2mdGPvd1P/z7Toebjm+qxTcdlmXXAgpb5+WVMkjRDpiPcvwPsExF7R8R2wPHAldOwH0nSOLp+WSYzN0XEKcBXaG6F/Exm3tnt/Uyjvvpxuss8tq1X7f8+NR/flI6t679QlST1nu9QlaQKGe6SVCHDvUVEbBMRt0XEVb2updsiYk5EXBoR/xcRKyPikF7X1C0R8Y6IuDMi7oiICyNi+17X1G9q7W37enyG++beDqzsdRHT5Ezgy5n5QuAlVHKcETEPeBuwMDN/n+aX+Mf3tqq+VGtv29fjMNyLiJgPHAWc0+taui0idgFeAZwLkJlPZubDPS2quwaAHSJiANgR+FmP6+krtfa2fT0xw/03zgDeDTzd4zqmw97AMPDZ8qP5ORGxU6+L6obMXAd8AvgpcC+wMTO/2tuq+s4Z1Nnb9vUEDHcgIo4G1mfmLb2uZZoMAAcAZ2Xm/sBjwLLeltQdEfEcmgfT7Q3sCewUEW/sbVX9o/Letq8nYLg3Xga8JiJW0zzF8tCI+HxvS+qqtcDazLy5zF9K85+iBq8GfpKZw5n5FHAZ8Mc9rqmf1Nzb9vUEDHcgM0/NzPmZOUTzS4vrMrOas7/MvA9YExEvKEOHAXf1sKRu+ilwcETsGBFBc2xV/FKtG2rubft6Yj17KqRm3FuBL5Tn/dwNnNTjeroiM2+OiEuBW4FNwG3U/VZ0bc6+HoePH5CkCnlZRpIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekCv0/osMhKcCYjJ0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_features = ['volatile acidity','alcohol','sulphates','citric acid']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2)\n",
    "print(X_norm2)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test =train_test_split(wine_df.drop('quality',axis=1), wine_df['quality'], test_size=.3,\n",
    " #                                                  random_state=22)\n",
    "#X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "def compute_cost_softmax(X, y, W, B):\n",
    "    m, n = X.shape\n",
    "    print(\"n\",n)\n",
    "    nb_f=W.shape[0]\n",
    "    print(\"nf\",nb_f)\n",
    "   ### START CODE HERE ###\n",
    "    loss_sum = 0 \n",
    "\n",
    "    \n",
    "  # on calcule tous les zj=fwb\n",
    "   # Loop over each training example\n",
    "    for i in range(m): \n",
    "      f_WB=np.zeros(nb_f)\n",
    "      # Loop over each class\n",
    "      for loop in range(nb_f):\n",
    "        z_wb = 0 \n",
    "       # Loop over each feature\n",
    "        for j in range(n): \n",
    "\n",
    "             z_wb_ij = W[loop,j]*X[i,j]\n",
    "             z_wb += z_wb_ij \n",
    "        z_wb += B[loop] \n",
    "\n",
    "        f_WB[loop] = sigmoid(z_wb)#=e(Zij), #à diviser par sumezi pour avoir probabilité que y==loop\n",
    "\n",
    "      sumEzi=sum(f_WB)\n",
    "      \n",
    "      loss_sum += np.log(f_WB[y[i]]/sumEzi) # on ajoute -log(a_i) if y=i\n",
    "\n",
    "    total_cost = -(1 / m) * loss_sum  \n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def compute_gradient_softmax(X, y, W, B): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    m, n = X.shape\n",
    "    nb_f=W.shape[0]\n",
    "\n",
    "    dJ_DW = np.zeros((nb_f,n))                           #(n,)\n",
    "    dJ_DB = np.zeros((nb_f))\n",
    "\n",
    "    for i in range(m): \n",
    "      f_WB=np.zeros(nb_f)\n",
    "      # Loop over each class\n",
    "      for loop in range(nb_f):\n",
    "        z_wb = 0 \n",
    "       # Loop over each feature\n",
    "        for j in range(n): \n",
    "\n",
    "             z_wb_ij = W[loop,j]*X[i,j]\n",
    "             z_wb += z_wb_ij \n",
    "        z_wb += B[loop] \n",
    "\n",
    "        f_WB[loop] = sigmoid(z_wb)#=e(Zij)\n",
    "\n",
    "      sumEzi=sum(f_WB) \n",
    "      f_WB=f_WB/sumEzi # tableau des probabilité que y==loop\n",
    "\n",
    "      #on calcule la dérivé\n",
    "      for loop in range(nb_f):\n",
    "        err_loop  = f_WB[loop]  - (y[loop]==y[i])         #scalar, proba que y = loop  - (1 ou 0)(si y ==loop)\n",
    "        for j in range(n):\n",
    "            dJ_DW[j] = dJ_DW[j] + err_loop * X[i,j]      #scalar\n",
    "        dJ_DB[loop] = dJ_DB[loop] + err_loop\n",
    "      dJ_DW = dJ_DW/m                                   #(n,)\n",
    "      dJ_DB = dJ_DB/m                                   #scalar\n",
    "        \n",
    "    return dJ_DB, dJ_DW \n",
    "\n",
    "def gradient_descent_softmax(X, y, w_in, b_in, alpha, num_iters, nbFeature): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    B = B_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_softmax(X, y, W, B)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_softmax(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_feature(X1, X2):\n",
    "    X1 = np.atleast_1d(X1)\n",
    "    X2 = np.atleast_1d(X2)\n",
    "    degree = 6\n",
    "    out = []\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(i + 1):\n",
    "            out.append((X1**(i-j) * (X2**j)))\n",
    "    return np.stack(out, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test cout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[-0.082978    0.22032449 -0.49988563 -0.19766743]\n",
      " [-0.35324411 -0.40766141 -0.31373979 -0.15443927]\n",
      " [-0.10323253  0.03881673 -0.08080549  0.1852195 ]\n",
      " [-0.29554775  0.37811744 -0.47261241  0.17046751]\n",
      " [-0.0826952   0.05868983 -0.35961306 -0.30189851]\n",
      " [ 0.30074457  0.46826158 -0.18657582  0.19232262]\n",
      " [ 0.37638915  0.39460666 -0.41495579 -0.46094522]\n",
      " [-0.33016958  0.3781425  -0.40165317 -0.07889237]\n",
      " [ 0.45788953  0.03316528  0.19187711 -0.18448437]]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "n 4\n",
      "nf 9\n",
      " cost : 2.2211481414984697\n"
     ]
    }
   ],
   "source": [
    "#[df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_train=np.copy(X_norm2)\n",
    "y_train=np.copy(y_train2)\n",
    "nbClasses=9\n",
    "print(X_train.shape[1])\n",
    "\n",
    "np.random.seed(1)\n",
    "initial_W = np.random.rand(nbClasses,X_train.shape[1]) - 0.5\n",
    "print(initial_W)\n",
    "initial_B = np.ones(nbClasses)*0.5\n",
    "print(initial_B)\n",
    "cost = compute_cost_softmax(X_train, y_train, initial_W, initial_B)\n",
    "\n",
    "print(\" cost :\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.71655875 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 0.66086539 -0.4087107  -1.10206203 -1.36502663]\n",
      " [ 0.27101182 -0.87093663 -0.51494625 -1.26329712]\n",
      " ...\n",
      " [-0.11884175  0.51574115  0.54186214 -0.70378484]\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "[[-0.082978    0.22032449 -0.49988563 -0.19766743]\n",
      " [-0.35324411 -0.40766141 -0.31373979 -0.15443927]\n",
      " [-0.10323253  0.03881673 -0.08080549  0.1852195 ]\n",
      " [-0.29554775  0.37811744 -0.47261241  0.17046751]\n",
      " [-0.0826952   0.05868983 -0.35961306 -0.30189851]\n",
      " [ 0.30074457  0.46826158 -0.18657582  0.19232262]\n",
      " [ 0.37638915  0.39460666 -0.41495579 -0.46094522]\n",
      " [-0.33016958  0.3781425  -0.40165317 -0.07889237]\n",
      " [ 0.45788953  0.03316528  0.19187711 -0.18448437]]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(initial_B)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lambda_ \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dj_db, dj_dw \u001b[39m=\u001b[39m compute_gradient_softmax(X_train, y_train, initial_W, initial_B)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdj_db: \u001b[39m\u001b[39m{\u001b[39;00mdj_db\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFirst few elements of dj_dw:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdj_dw[:\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, )\n",
      "\u001b[1;32m/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb Cell 7\u001b[0m in \u001b[0;36mcompute_gradient_softmax\u001b[0;34m(X, y, W, B)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m dj_db \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     f_wb_i \u001b[39m=\u001b[39m sigmoid(np\u001b[39m.\u001b[39mdot(X[i],w) \u001b[39m+\u001b[39m b)          \u001b[39m#(n,)(n,)=scalar\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     err_i  \u001b[39m=\u001b[39m f_wb_i  \u001b[39m-\u001b[39m y[i]                       \u001b[39m#scalar\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/henri/ZZZ2/wineQualityPrediction/3.5Softmax.ipynb#X50sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "\n",
    "np.random.seed(1)\n",
    "initial_W = np.random.rand(nbClasses,X_train.shape[1]) - 0.5\n",
    "print(initial_W)\n",
    "initial_B = np.ones(nbClasses)*0.5\n",
    "print(initial_B)\n",
    " \n",
    "lambda_ = 0.5\n",
    "dJ_DB, dJ_DW = compute_gradient_softmax(X_train, y_train, initial_W, initial_B)\n",
    "\n",
    "print(f\"dj_db: {dJ_DB}\", )\n",
    "print(f\"First few elements of dj_dw:\\n {dJ_DW[:4].tolist()}\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
