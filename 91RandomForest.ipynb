{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARBRES DE DECISION: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 1.94181282 -0.59360107  0.1308811  -1.36502663]\n",
      " [ 1.27349242 -0.59360107 -0.04525363 -1.16156762]\n",
      " ...\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "(592,)\n",
      "(592, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_features = ['volatile acidity','alcohol','sulphates','citric acid']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2.shape)\n",
    "print(X_norm2.shape)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test =train_test_split(wine_df.drop('quality',axis=1), wine_df['quality'], test_size=.3,\n",
    " #                                                  random_state=22)\n",
    "#X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1- p)*np.log2(1 - p)\n",
    "    \n",
    "print(entropy(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def split_indices_continue(X,t, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[0,index_feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7    9.4    0.56   0.   ]\n",
      " [ 0.88   9.8    0.68   0.   ]\n",
      " [ 0.76   9.8    0.65   0.04 ]\n",
      " ...\n",
      " [ 0.6   10.5    0.58   0.08 ]\n",
      " [ 0.55  11.2    0.76   0.1  ]\n",
      " [ 0.645 10.2    0.71   0.12 ]]\n",
      "536 607\n"
     ]
    }
   ],
   "source": [
    "left,right=split_indices_continue(X_train,0.5, 0)\n",
    "print(X_train)\n",
    "print(len(left),len(right))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut utiliser l'entrepie, mais on va utilsier le critère de gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X,y,left_indices,right_indices):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "    p_left = sum(y[left_indices])/len(left_indices)\n",
    "    print(p_left)\n",
    "    p_right = sum(y[right_indices])/len(right_indices)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "\n",
    "    #calcul p_left et p_right : moyenne de Y>threshold_y(qualité du vin) (moyenne d'un tableau de 1 0 1 00 1...)\n",
    "    p_left=np.mean(y[left_indices]>=threshold_y)\n",
    "    p_right=np.mean(y[right_indices]>=threshold_y)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def gini_Impurity(X,y,left_indices,right_indices,threshold_y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   6.  33. 483. 462. 143.  16.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6413461221984171"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_indices, right_indices = split_indices_continue(X_train,0.5, 0)\n",
    "gini_Impurity(X_train, y_train, left_indices, right_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node = sum(y)/len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X,y,left_indices,right_indices)\n",
    "    return h_node - w_entropy\n",
    "\n",
    "def information_gain_continue(X, y, left_indices, right_indices, threshold_y):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node=np.mean(y[left_indices]>=threshold_y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1122412453214347"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain_continue(X_train, y_train, left_indices, right_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: volatile acidity, information gain if we split the root node using this feature: -0.10\n",
      "Feature: alcohol, information gain if we split the root node using this feature: nan\n",
      "Feature: sulphates, information gain if we split the root node using this feature: 0.14\n",
      "Feature: citric acid, information gain if we split the root node using this feature: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/henri/.local/lib/python3.8/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for i, feature_name in enumerate(X_features):\n",
    "    left_indices, right_indices = split_indices_continue(X_train, 0.5,i)\n",
    "    i_gain = information_gain_continue(X_train, y_train, left_indices, right_indices,5)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "\n",
    "    entropy = 0\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    entropy = sum(y[y==1])/len(y)\n",
    "    if entropy == 0 or entropy == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -entropy*np.log2(entropy) - (1-entropy)*np.log2(1-entropy)\n",
    "     \n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    for i in node_indices:\n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "    print(\"max_info_gain\",max_info_gain)\n",
    "   \n",
    "    return best_feature\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for multi classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_Impurity(X,y,left_indices,right_indices,threshold_y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "\n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    for i in node_indices:\n",
    "        if X[i][feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "def compute_information_gain_continue(X, y, node_indices, feature):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,0.5)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "    print(\"max_info_gain\",max_info_gain)\n",
    "   \n",
    "    return best_feature\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 0\n",
      " - Left leaf node with indices [0, 3, 4, 5, 7]\n",
      " - Right leaf node with indices [1, 2, 6, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0, 3, 4, 5, 7], [1, 2, 6, 8, 9], 0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, [0,1,2,3,4,5,6,7,8,9], \"Root\", max_depth=1, current_depth=0, tree = tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_info_gain 0.2780719051126377\n",
      " Depth 0, Root: Split on feature: 0\n",
      "max_info_gain 0.7219280948873623\n",
      "- Depth 1, Left: Split on feature: 1\n",
      "max_info_gain 0\n",
      "-- Depth 2, Left: Split on feature: -1\n",
      "   --- Left leaf node with indices [0, 4, 5, 7]\n",
      "   --- Right leaf node with indices []\n",
      "max_info_gain 0\n",
      "-- Depth 2, Right: Split on feature: -1\n",
      "   --- Left leaf node with indices [3]\n",
      "   --- Right leaf node with indices []\n",
      "max_info_gain 0.7219280948873623\n",
      "- Depth 1, Right: Split on feature: 2\n",
      "max_info_gain 0\n",
      "-- Depth 2, Left: Split on feature: -1\n",
      "   --- Left leaf node with indices [1]\n",
      "   --- Right leaf node with indices []\n",
      "max_info_gain 0\n",
      "-- Depth 2, Right: Split on feature: -1\n",
      "   --- Left leaf node with indices [2, 6, 8, 9]\n",
      "   --- Right leaf node with indices []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0, 3, 4, 5, 7], [1, 2, 6, 8, 9], 0),\n",
       " ([0, 4, 5, 7], [3], 1),\n",
       " ([0, 4, 5, 7], [], -1),\n",
       " ([3], [], -1),\n",
       " ([1], [2, 6, 8, 9], 2),\n",
       " ([1], [], -1),\n",
       " ([2, 6, 8, 9], [], -1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, [0,1,2,3,4,5,6,7,8,9], \"Root\", max_depth=3, current_depth=0, tree = tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREE ENSEMBLES : RANDOM FOREST POUR LA QUALITE DU VIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 1.94181282 -0.59360107  0.1308811  -1.36502663]\n",
      " [ 1.27349242 -0.59360107 -0.04525363 -1.16156762]\n",
      " ...\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "(559,)\n",
      "(559, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_features = ['volatile acidity','alcohol','sulphates','citric acid']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2.shape)\n",
    "print(X_norm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n",
      "[5 6 5 7 7 5 7 5 5 5 6 5 6 6 7 4 6 4 5 6 5 5 4 5 6 5 4 5 5 5 4 5 5 5 5 5 6\n",
      " 6 5 5 7 5 5 5 5 5 6 4 5 5 5 4 6 5 4 4 6 5 5 6 5 5 6 4 7 7 7 5 5 6 7 5 7 7\n",
      " 6 5 5 5 7 5 4 4 8 6 6 6 8 7 7 7 6 7 7 6 6 5 5 5 5 6 6 5 7 7 6 7 7 6 5 7 6\n",
      " 6 5 5 6 6 7 7 7 7 5 7 7 6 6 8 5 7 6 5 5 6 7 4 6 5 7 7 7 7 7 5 6 8 7 7 5 5\n",
      " 6 6 7 8 5 3 5 6 5 6 5 8 6 6 7 7 6 8 5 8 6 6 7 7 7 7 7 6 7 6 6 7 7 5 6 3 5\n",
      " 5 6 5 5 6 7 5 5 6 5 6 6 6 5 5 5 6 6 6 6 4 5 5 6 5 7 8 6 5 6 5 6 6 5 5 5 5\n",
      " 6 6 6 4 5 4 7 5 5 7 6 5 5 5 5 5 5 5 5 6 6 4 4 5 6 5 5 5 5 6 5 5 5 5 5 6 5\n",
      " 5 6 6 5 6 5 6 5 5 5 6 6 5 6 5 7 5 7 7 5 6 4 7 5 7 4 4 7 7 7 5 5 6 7 7 5 5\n",
      " 4 7 6 6 6 6 7 7 7 7 7 6 6 6 6 5 7 4 5 7 7 7 7 7 7 7 7 7 7 7 7 6 6 5 6 6 7\n",
      " 5 7 6 5 5 7 7 7 7 7 7 5 6 7 6 5 6 7 7 5 7 7 7 6 6 6 5 5 7 6 7 5 7 7 8 6 7\n",
      " 7 7 5 7 7 7 7 7 8 6 7 6 5 6 6 6 6 7 5 7 8 7 5 7 7 7 6 5 6 6 7 6 6 6 7 7 7\n",
      " 7 6 6 4 6 5 4 5 7 6 6 7 8 7 7 5 7 7 6 6 6 6 6 5 5 7 6 6 4 4 5 5 5 5 6 4 6\n",
      " 6 6 6 5 6 4 7 6 6 6 5 5 5 5 3 5 6 6 6 6 6 5 5 6 5 6 6 5 5 5 5 6 5 6 6 6 5\n",
      " 6 5 5 5 8 6 7 6 6 6 5 7 5 5 4 5 5 6 7 8 7 7 7 6 5 7 4 5 7 4 7 3 5 5 5 7 7\n",
      " 3 5 4 5 5 5 6 5 7 6 6 3 6 5 6 5 6 6 5 5 5 6 5 7 6 5 7 5 8 5 6 7 5 6 5 5 5\n",
      " 7 6 6 5]\n",
      "[2 3 2 4 4 2 4 2 2 2 3 2 3 3 4 1 3 1 2 3 2 2 1 2 3 2 1 2 2 2 1 2 2 2 2 2 3\n",
      " 3 2 2 4 2 2 2 2 2 3 1 2 2 2 1 3 2 1 1 3 2 2 3 2 2 3 1 4 4 4 2 2 3 4 2 4 4\n",
      " 3 2 2 2 4 2 1 1 5 3 3 3 5 4 4 4 3 4 4 3 3 2 2 2 2 3 3 2 4 4 3 4 4 3 2 4 3\n",
      " 3 2 2 3 3 4 4 4 4 2 4 4 3 3 5 2 4 3 2 2 3 4 1 3 2 4 4 4 4 4 2 3 5 4 4 2 2\n",
      " 3 3 4 5 2 0 2 3 2 3 2 5 3 3 4 4 3 5 2 5 3 3 4 4 4 4 4 3 4 3 3 4 4 2 3 0 2\n",
      " 2 3 2 2 3 4 2 2 3 2 3 3 3 2 2 2 3 3 3 3 1 2 2 3 2 4 5 3 2 3 2 3 3 2 2 2 2\n",
      " 3 3 3 1 2 1 4 2 2 4 3 2 2 2 2 2 2 2 2 3 3 1 1 2 3 2 2 2 2 3 2 2 2 2 2 3 2\n",
      " 2 3 3 2 3 2 3 2 2 2 3 3 2 3 2 4 2 4 4 2 3 1 4 2 4 1 1 4 4 4 2 2 3 4 4 2 2\n",
      " 1 4 3 3 3 3 4 4 4 4 4 3 3 3 3 2 4 1 2 4 4 4 4 4 4 4 4 4 4 4 4 3 3 2 3 3 4\n",
      " 2 4 3 2 2 4 4 4 4 4 4 2 3 4 3 2 3 4 4 2 4 4 4 3 3 3 2 2 4 3 4 2 4 4 5 3 4\n",
      " 4 4 2 4 4 4 4 4 5 3 4 3 2 3 3 3 3 4 2 4 5 4 2 4 4 4 3 2 3 3 4 3 3 3 4 4 4\n",
      " 4 3 3 1 3 2 1 2 4 3 3 4 5 4 4 2 4 4 3 3 3 3 3 2 2 4 3 3 1 1 2 2 2 2 3 1 3\n",
      " 3 3 3 2 3 1 4 3 3 3 2 2 2 2 0 2 3 3 3 3 3 2 2 3 2 3 3 2 2 2 2 3 2 3 3 3 2\n",
      " 3 2 2 2 5 3 4 3 3 3 2 4 2 2 1 2 2 3 4 5 4 4 4 3 2 4 1 2 4 1 4 0 2 2 2 4 4\n",
      " 0 2 1 2 2 2 3 2 4 3 3 0 3 2 3 2 3 3 2 2 2 3 2 4 3 2 4 2 5 2 3 4 2 3 2 2 2\n",
      " 4 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "X_train=X_norm2\n",
    "y_train=y_train2\n",
    "n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval\n",
    "print(int(len(y_train)*0.8)) ## Let's use 80% to train and 20% to eval\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(y_train)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.71506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-mlogloss:1.65175\n",
      "[2]\tvalidation_0-mlogloss:1.59988\n",
      "[3]\tvalidation_0-mlogloss:1.56101\n",
      "[4]\tvalidation_0-mlogloss:1.52425\n",
      "[5]\tvalidation_0-mlogloss:1.49162\n",
      "[6]\tvalidation_0-mlogloss:1.46611\n",
      "[7]\tvalidation_0-mlogloss:1.44765\n",
      "[8]\tvalidation_0-mlogloss:1.43042\n",
      "[9]\tvalidation_0-mlogloss:1.41832\n",
      "[10]\tvalidation_0-mlogloss:1.40874\n",
      "[11]\tvalidation_0-mlogloss:1.40257\n",
      "[12]\tvalidation_0-mlogloss:1.39479\n",
      "[13]\tvalidation_0-mlogloss:1.38973\n",
      "[14]\tvalidation_0-mlogloss:1.38570\n",
      "[15]\tvalidation_0-mlogloss:1.37859\n",
      "[16]\tvalidation_0-mlogloss:1.37371\n",
      "[17]\tvalidation_0-mlogloss:1.36740\n",
      "[18]\tvalidation_0-mlogloss:1.35963\n",
      "[19]\tvalidation_0-mlogloss:1.35460\n",
      "[20]\tvalidation_0-mlogloss:1.35176\n",
      "[21]\tvalidation_0-mlogloss:1.34938\n",
      "[22]\tvalidation_0-mlogloss:1.34726\n",
      "[23]\tvalidation_0-mlogloss:1.34511\n",
      "[24]\tvalidation_0-mlogloss:1.34286\n",
      "[25]\tvalidation_0-mlogloss:1.34107\n",
      "[26]\tvalidation_0-mlogloss:1.34306\n",
      "[27]\tvalidation_0-mlogloss:1.34294\n",
      "[28]\tvalidation_0-mlogloss:1.34530\n",
      "[29]\tvalidation_0-mlogloss:1.34635\n",
      "[30]\tvalidation_0-mlogloss:1.34757\n",
      "[31]\tvalidation_0-mlogloss:1.34970\n",
      "[32]\tvalidation_0-mlogloss:1.35345\n",
      "[33]\tvalidation_0-mlogloss:1.35611\n",
      "[34]\tvalidation_0-mlogloss:1.35862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8211\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8211\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
