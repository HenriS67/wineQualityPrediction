{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARBRES DE DECISION: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 1.94181282 -0.59360107  0.1308811  -1.36502663]\n",
      " [ 1.27349242 -0.59360107 -0.04525363 -1.16156762]\n",
      " ...\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "[5 6 5 7 7 5 5 5 7 5 5 6 5 6 7 5 4 5 4 5 6 6 6 5 5 5 6 5 4 5 6 4 6 5 4 5 6\n",
      " 6 6 5 5 5 6 5 5 5 5 7 5 5 6 5 6 6 5 6 4 5 5 5 5 4 6 5 4 6 4 6 6 5 5 5 6 5\n",
      " 5 5 5 5 6 4 7 6 7 7 6 5 5 6 6 7 6 5 7 7 6 6 6 5 5 5 7 4 5 4 8 6 8 7 7 7 5\n",
      " 6 7 7 6 6 6 6 5 5 6 6 6 5 7 6 6 7 7 7 7 6 6 7 7 5 5 7 7 5 7 6 6 7 6 8 7 5\n",
      " 5 6 6 7 4 6 7 5 5 5 6 7 5 7 7 6 5 7 6 5 6 8 7 7 6 7 8 5 3 6 5 6 6 5 5 8 5\n",
      " 6 6 7 7 6 6 8 5 8 6 7 7 7 7 7 7 7 7 3 6 5 6 5 7 5 6 6 6 5 6 6 6 6 4 5 5 7\n",
      " 5 8 6 5 6 6 5 5 6 5 5 5 6 6 5 4 5 4 7 5 7 6 5 6 5 5 5 5 5 5 6 6 6 4 4 5 5\n",
      " 5 5 5 5 5 5 6 5 5 5 5 6 5 5 6 5 5 5 6 5 5 6 5 5 5 6 5 7 5 7 6 7 5 5 4 5 7\n",
      " 5 7 4 4 7 7 7 6 6 5 5 5 6 7 7 5 5 4 7 6 6 5 5 6 6 7 7 5 7 7 7 5 6 6 6 6 6\n",
      " 5 6 6 6 7 6 4 5 7 5 6 5 7 7 7 7 7 7 7 7 7 7 5 7 5 6 5 7 5 5 7 6 6 5 5 6 5\n",
      " 5 6 7 7 7 7 7 7 5 7 5 6 7 5 7 5 6 7 7 7 6 6 6 6 5 6 7 6 7 7 7 6 8 7 7 5 7\n",
      " 6 5 7 7 7 7 7 8 6 7 6 6 6 7 7 6 6 8 7 6 7 7 7 6 6 6 6 5 6 7 5 7 7 7 7 6 6\n",
      " 6 6 4 6 5 4 5 7 6 6 7 8 7 7 5 7 7 6 6 6 5 5 7 5 4 4 5 5 4 6 6 6 6 5 4 6 7\n",
      " 6 5 5 5 6 6 3 6 5 6 6 6 5 6 6 5 5 5 6 6 5 6 5 6 6 6 5 6 5 5 5 5 8 6 7 5 5\n",
      " 7 5 5 4 5 7 5 5 5 6 5 8 7 7 7 6 6 5 5 7 4 6 7 4 7 3 5 6 7 7 3 5 4 5 5 5 5\n",
      " 5 5 7 6 6 5 6 3 6 6 5 6 5 6 5 5 6 6 7 5 7 5 8 7 5 5 5 5 5 6 5 5 6 6 5 7 6\n",
      " 6 6 6 6]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [-1.39978919 -0.59360107 -0.45623467  1.48339955]\n",
      " [ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " ...\n",
      " [ 1.16210569  1.07041225 -0.57365783 -0.90724385]\n",
      " [ 0.49378528 -0.87093663  0.95284319 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_features = ['volatile acidity','alcohol','sulphates','citric acid']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2)\n",
    "print(X_norm2)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test =train_test_split(wine_df.drop('quality',axis=1), wine_df['quality'], test_size=.3,\n",
    " #                                                  random_state=22)\n",
    "#X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1- p)*np.log2(1 - p)\n",
    "    \n",
    "print(entropy(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[index_feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n",
    "\n",
    "def split_indices_continue(X,t, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate(X):\n",
    "        if x[0,index_feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [-1.39978919 -0.59360107 -0.45623467  1.48339955]\n",
      " [ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " ...\n",
      " [ 1.16210569  1.07041225 -0.57365783 -0.90724385]\n",
      " [ 0.49378528 -0.87093663  0.95284319 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]]\n",
      "450 146\n"
     ]
    }
   ],
   "source": [
    "X_train=X_norm2\n",
    "y_train=y_train2\n",
    "left,right=split_indices_continue(X_train,0.5, 0)\n",
    "print(X_train)\n",
    "print(len(left),len(right))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on peut utiliser l'entrepie, mais on va utilsier le critère de gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X,y,left_indices,right_indices):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "    p_left = sum(y[left_indices])/len(left_indices)\n",
    "    print(p_left)\n",
    "    p_right = sum(y[right_indices])/len(right_indices)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "\n",
    "    #calcul p_left et p_right : moyenne de Y>threshold_y(qualité du vin) (moyenne d'un tableau de 1 0 1 00 1...)\n",
    "    p_left=np.mean(y[left_indices]>=threshold_y)\n",
    "    p_right=np.mean(y[right_indices]>=threshold_y)\n",
    "    \n",
    "    weighted_entropy = w_left * entropy(p_left) + w_right * entropy(p_right)\n",
    "    return weighted_entropy\n",
    "\n",
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   6.  33. 210. 188. 143.  16.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7148945993423721"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_indices, right_indices = split_indices_continue(X_train,0.5, 0)\n",
    "gini_Impurity(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node = sum(y)/len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X,y,left_indices,right_indices)\n",
    "    return h_node - w_entropy\n",
    "\n",
    "def information_gain_continue(X, y, left_indices, right_indices, threshold_y):\n",
    "    \"\"\"\n",
    "    Here, X has the elements in the node and y is theirs respectives classes\n",
    "    \"\"\"\n",
    "    p_node=np.mean(y[left_indices]>=threshold_y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy_continue(X,y,left_indices,right_indices,threshold_y)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10627911837247811"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain_continue(X_train, y_train, left_indices, right_indices,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: volatile acidity, information gain if we split the root node using this feature: -0.11\n",
      "Feature: alcohol, information gain if we split the root node using this feature: 0.03\n",
      "Feature: sulphates, information gain if we split the root node using this feature: 0.06\n",
      "Feature: citric acid, information gain if we split the root node using this feature: 0.06\n"
     ]
    }
   ],
   "source": [
    "for i, feature_name in enumerate(X_features):\n",
    "    left_indices, right_indices = split_indices_continue(X_train, 0.5,i)\n",
    "    i_gain = information_gain_continue(X_train, y_train, left_indices, right_indices,5)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "\n",
    "    entropy = 0\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    entropy = sum(y[y==1])/len(y)\n",
    "    if entropy == 0 or entropy == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return -entropy*np.log2(entropy) - (1-entropy)*np.log2(1-entropy)\n",
    "     \n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "\n",
    "    for i in node_indices:\n",
    "        if X[i][feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    for feature in range(num_features):\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "    print(\"max_info_gain\",max_info_gain)\n",
    "   \n",
    "    return best_feature\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for multi classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_Impurity(y):\n",
    "    \"\"\"\n",
    "    This function takes the splitted dataset, the indices we chose to split and returns the weighted entropy.\n",
    "    \"\"\"\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    #print(tab_value)\n",
    "\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "    \n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    #print(\"t\",t)\n",
    "    for i in node_indices:\n",
    "        #print(X[i,feature])\n",
    "        if X[i,feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "def compute_information_gain_continue(X, y, node_indices, feature, t):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,t)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = gini_Impurity(y_node)\n",
    "    left_entropy = gini_Impurity(y_left)\n",
    "    right_entropy = gini_Impurity(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    tmax=0\n",
    "\n",
    "    tab_max_feature=np.zeros(num_features)\n",
    "    tab_min_feature=np.zeros(num_features)\n",
    "    for loop in range(num_features):\n",
    "        tab_max_feature[loop]=np.max(np.transpose(X)[loop])\n",
    "        tab_min_feature[loop]=np.min(np.transpose(X)[loop])\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        tab_t_feature=np.linspace(tab_min_feature[feature], tab_max_feature[feature], len(X)-1)\n",
    "        for t in range(len(tab_t_feature)):\n",
    "            #print(t,X_features[feature])\n",
    "            info_gain = compute_information_gain_continue(X, y, node_indices, feature,tab_t_feature[t])\n",
    "            #print(tab_t_feature[t],X_features[feature],info_gain)\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                tmax=tab_t_feature[t]\n",
    "    #print(\"max_info_gain\",max_info_gain,X_features[best_feature],tmax)\n",
    "   \n",
    "    return best_feature,tmax,max_info_gain\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        print(formatting,\"note moyenne attribuée à la feuille :\",np.mean(y[node_indices]),\"(\",round(np.mean(y[node_indices])),\")\")\n",
    "        return 0\n",
    "   \n",
    "\n",
    "    best_feature,tmax,max_info = get_best_split_continue(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %s <= %s, pour un gain de %s\" % (formatting, current_depth, branch_name, X_features[best_feature], tmax,max_info))\n",
    "\n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, best_feature,tmax)\n",
    "    tree.append((left_indices, right_indices, best_feature,tmax))\n",
    "    \n",
    "    build_tree_recursive_continue(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree)\n",
    "    build_tree_recursive_continue(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [-1.39978919 -0.59360107 -0.45623467  1.48339955]\n",
      " [ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " ...\n",
      " [ 1.16210569  1.07041225 -0.57365783 -0.90724385]\n",
      " [ 0.49378528 -0.87093663  0.95284319 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31902/1131864183.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  impurity-=(tab_value[loop]/sum(tab_value))**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: alcohol <= 0.23871685382930918, pour un gain de 0.07487931829161631\n",
      "- Depth 1, Left: Split on feature: sulphates <= -0.6200142753667681, pour un gain de 0.03768815421926175\n",
      "-- Depth 2, Left: Split on feature: volatile acidity <= 1.8979332013335686, pour un gain de 0.019757375114668296\n",
      "   --- Left leaf node with indices [3, 5, 6, 12, 16, 22, 27, 28, 30, 39, 40, 44, 54, 57, 58, 63, 64, 65, 73, 74, 76, 85, 119, 122, 123, 147, 148, 157, 167, 184, 185, 218, 219, 220, 227, 228, 229, 231, 233, 246, 247, 256, 258, 259, 261, 262, 263, 268, 274, 276, 277, 278, 279, 283, 287, 292, 294, 296, 306, 307, 311, 312, 313, 316, 359, 361, 362, 366, 367, 369, 370, 385, 386, 437, 453, 464, 468, 479, 493, 495, 496, 497, 498, 499, 500, 501, 503, 510, 512, 519, 526, 544, 553, 560, 580, 581, 582, 583, 585]\n",
      "   --- note moyenne attribuée à la feuille : 5.090909090909091 ( 5 )\n",
      "   --- Right leaf node with indices [34, 50, 66, 257, 269, 281, 378, 449, 543, 548]\n",
      "   --- note moyenne attribuée à la feuille : 4.4 ( 4 )\n",
      "-- Depth 2, Right: Split on feature: volatile acidity <= 0.07730391112777513, pour un gain de 0.027804149051658067\n",
      "   --- Left leaf node with indices [1, 8, 9, 10, 14, 15, 20, 21, 23, 29, 32, 33, 36, 38, 42, 51, 55, 56, 59, 60, 62, 67, 68, 75, 78, 80, 81, 87, 90, 92, 93, 95, 99, 100, 102, 105, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 126, 130, 134, 141, 149, 152, 153, 155, 158, 164, 166, 169, 172, 179, 180, 182, 186, 194, 204, 205, 206, 207, 210, 211, 215, 221, 225, 226, 230, 234, 235, 236, 238, 242, 243, 245, 249, 250, 251, 255, 264, 272, 280, 282, 285, 289, 291, 299, 310, 338, 346, 355, 364, 380, 387, 391, 394, 401, 412, 413, 431, 432, 434, 435, 444, 445, 463, 481, 492, 505, 513, 517, 520, 523, 524, 535, 536, 540, 542, 549, 551, 552, 556, 557, 558, 566, 569, 570, 571, 576]\n",
      "   --- note moyenne attribuée à la feuille : 5.826086956521739 ( 6 )\n",
      "   --- Right leaf node with indices [0, 2, 4, 7, 11, 17, 19, 24, 25, 26, 31, 35, 37, 41, 43, 47, 48, 61, 69, 70, 71, 72, 77, 86, 88, 91, 94, 96, 97, 98, 101, 103, 110, 118, 125, 131, 132, 135, 136, 137, 138, 139, 156, 163, 175, 176, 178, 181, 203, 209, 214, 222, 232, 241, 244, 253, 254, 260, 266, 267, 270, 271, 275, 304, 305, 317, 318, 319, 320, 339, 342, 357, 371, 382, 384, 397, 407, 452, 458, 461, 465, 470, 471, 474, 483, 484, 489, 491, 494, 507, 509, 511, 516, 528, 538, 550, 554, 562, 567, 568, 578, 579, 586, 594]\n",
      "   --- note moyenne attribuée à la feuille : 5.278846153846154 ( 5 )\n",
      "- Depth 1, Right: Split on feature: volatile acidity <= -1.1136340456233826, pour un gain de 0.053888364188867555\n",
      "-- Depth 2, Left: Split on feature: volatile acidity <= -1.9486595095523556, pour un gain de 0.014442487467822784\n",
      "   --- Left leaf node with indices [350, 351, 438, 522, 565]\n",
      "   --- note moyenne attribuée à la feuille : 6.2 ( 6 )\n",
      "   --- Right leaf node with indices [82, 83, 106, 140, 143, 161, 165, 173, 183, 197, 199, 200, 284, 290, 297, 300, 301, 302, 309, 314, 321, 322, 323, 331, 337, 341, 343, 345, 347, 349, 352, 353, 360, 368, 374, 375, 377, 381, 388, 396, 404, 406, 409, 410, 414, 416, 420, 422, 425, 429, 440, 441, 451, 462, 480, 514, 515, 531, 537, 590, 591]\n",
      "   --- note moyenne attribuée à la feuille : 6.868852459016393 ( 7 )\n",
      "-- Depth 2, Right: Split on feature: sulphates <= -0.042288399394877496, pour un gain de 0.04018918907739821\n",
      "   --- Left leaf node with indices [13, 18, 45, 46, 49, 79, 89, 146, 150, 151, 159, 160, 162, 177, 190, 191, 193, 213, 216, 224, 237, 239, 248, 252, 273, 293, 298, 315, 326, 328, 329, 330, 332, 333, 344, 363, 373, 383, 392, 393, 398, 400, 405, 408, 415, 417, 419, 421, 423, 426, 433, 436, 442, 443, 446, 448, 450, 454, 467, 469, 472, 477, 478, 482, 486, 487, 490, 502, 504, 506, 508, 525, 534, 539, 541, 555, 559, 564, 573, 574, 588, 589, 593]\n",
      "   --- note moyenne attribuée à la feuille : 5.674698795180723 ( 6 )\n",
      "   --- Right leaf node with indices [52, 53, 84, 104, 124, 127, 128, 129, 133, 142, 144, 145, 154, 168, 170, 171, 174, 187, 188, 189, 192, 195, 196, 198, 201, 202, 208, 212, 217, 223, 240, 265, 286, 288, 295, 303, 308, 324, 325, 327, 334, 335, 336, 340, 348, 354, 356, 358, 365, 372, 376, 379, 389, 390, 395, 399, 402, 403, 411, 418, 424, 427, 428, 430, 439, 447, 455, 456, 457, 459, 460, 466, 473, 475, 476, 485, 488, 518, 521, 527, 529, 530, 532, 533, 545, 546, 547, 561, 563, 572, 575, 577, 584, 587, 592, 595]\n",
      "   --- note moyenne attribuée à la feuille : 6.614583333333333 ( 7 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   47,\n",
       "   48,\n",
       "   50,\n",
       "   51,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   105,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   125,\n",
       "   126,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   141,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   152,\n",
       "   153,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   163,\n",
       "   164,\n",
       "   166,\n",
       "   167,\n",
       "   169,\n",
       "   172,\n",
       "   175,\n",
       "   176,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   194,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   214,\n",
       "   215,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   238,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   285,\n",
       "   287,\n",
       "   289,\n",
       "   291,\n",
       "   292,\n",
       "   294,\n",
       "   296,\n",
       "   299,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   310,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   338,\n",
       "   339,\n",
       "   342,\n",
       "   346,\n",
       "   355,\n",
       "   357,\n",
       "   359,\n",
       "   361,\n",
       "   362,\n",
       "   364,\n",
       "   366,\n",
       "   367,\n",
       "   369,\n",
       "   370,\n",
       "   371,\n",
       "   378,\n",
       "   380,\n",
       "   382,\n",
       "   384,\n",
       "   385,\n",
       "   386,\n",
       "   387,\n",
       "   391,\n",
       "   394,\n",
       "   397,\n",
       "   401,\n",
       "   407,\n",
       "   412,\n",
       "   413,\n",
       "   431,\n",
       "   432,\n",
       "   434,\n",
       "   435,\n",
       "   437,\n",
       "   444,\n",
       "   445,\n",
       "   449,\n",
       "   452,\n",
       "   453,\n",
       "   458,\n",
       "   461,\n",
       "   463,\n",
       "   464,\n",
       "   465,\n",
       "   468,\n",
       "   470,\n",
       "   471,\n",
       "   474,\n",
       "   479,\n",
       "   481,\n",
       "   483,\n",
       "   484,\n",
       "   489,\n",
       "   491,\n",
       "   492,\n",
       "   493,\n",
       "   494,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   503,\n",
       "   505,\n",
       "   507,\n",
       "   509,\n",
       "   510,\n",
       "   511,\n",
       "   512,\n",
       "   513,\n",
       "   516,\n",
       "   517,\n",
       "   519,\n",
       "   520,\n",
       "   523,\n",
       "   524,\n",
       "   526,\n",
       "   528,\n",
       "   535,\n",
       "   536,\n",
       "   538,\n",
       "   540,\n",
       "   542,\n",
       "   543,\n",
       "   544,\n",
       "   548,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   552,\n",
       "   553,\n",
       "   554,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   560,\n",
       "   562,\n",
       "   566,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   576,\n",
       "   578,\n",
       "   579,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   585,\n",
       "   586,\n",
       "   594],\n",
       "  [13,\n",
       "   18,\n",
       "   45,\n",
       "   46,\n",
       "   49,\n",
       "   52,\n",
       "   53,\n",
       "   79,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   89,\n",
       "   104,\n",
       "   106,\n",
       "   124,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   133,\n",
       "   140,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   150,\n",
       "   151,\n",
       "   154,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   165,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   173,\n",
       "   174,\n",
       "   177,\n",
       "   183,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   208,\n",
       "   212,\n",
       "   213,\n",
       "   216,\n",
       "   217,\n",
       "   223,\n",
       "   224,\n",
       "   237,\n",
       "   239,\n",
       "   240,\n",
       "   248,\n",
       "   252,\n",
       "   265,\n",
       "   273,\n",
       "   284,\n",
       "   286,\n",
       "   288,\n",
       "   290,\n",
       "   293,\n",
       "   295,\n",
       "   297,\n",
       "   298,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   308,\n",
       "   309,\n",
       "   314,\n",
       "   315,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   340,\n",
       "   341,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   347,\n",
       "   348,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   354,\n",
       "   356,\n",
       "   358,\n",
       "   360,\n",
       "   363,\n",
       "   365,\n",
       "   368,\n",
       "   372,\n",
       "   373,\n",
       "   374,\n",
       "   375,\n",
       "   376,\n",
       "   377,\n",
       "   379,\n",
       "   381,\n",
       "   383,\n",
       "   388,\n",
       "   389,\n",
       "   390,\n",
       "   392,\n",
       "   393,\n",
       "   395,\n",
       "   396,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   402,\n",
       "   403,\n",
       "   404,\n",
       "   405,\n",
       "   406,\n",
       "   408,\n",
       "   409,\n",
       "   410,\n",
       "   411,\n",
       "   414,\n",
       "   415,\n",
       "   416,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   420,\n",
       "   421,\n",
       "   422,\n",
       "   423,\n",
       "   424,\n",
       "   425,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   429,\n",
       "   430,\n",
       "   433,\n",
       "   436,\n",
       "   438,\n",
       "   439,\n",
       "   440,\n",
       "   441,\n",
       "   442,\n",
       "   443,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   450,\n",
       "   451,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   459,\n",
       "   460,\n",
       "   462,\n",
       "   466,\n",
       "   467,\n",
       "   469,\n",
       "   472,\n",
       "   473,\n",
       "   475,\n",
       "   476,\n",
       "   477,\n",
       "   478,\n",
       "   480,\n",
       "   482,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   490,\n",
       "   502,\n",
       "   504,\n",
       "   506,\n",
       "   508,\n",
       "   514,\n",
       "   515,\n",
       "   518,\n",
       "   521,\n",
       "   522,\n",
       "   525,\n",
       "   527,\n",
       "   529,\n",
       "   530,\n",
       "   531,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   537,\n",
       "   539,\n",
       "   541,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   555,\n",
       "   559,\n",
       "   561,\n",
       "   563,\n",
       "   564,\n",
       "   565,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   577,\n",
       "   584,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   590,\n",
       "   591,\n",
       "   592,\n",
       "   593,\n",
       "   595],\n",
       "  1,\n",
       "  0.23871685382930918),\n",
       " ([3,\n",
       "   5,\n",
       "   6,\n",
       "   12,\n",
       "   16,\n",
       "   22,\n",
       "   27,\n",
       "   28,\n",
       "   30,\n",
       "   34,\n",
       "   39,\n",
       "   40,\n",
       "   44,\n",
       "   50,\n",
       "   54,\n",
       "   57,\n",
       "   58,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   73,\n",
       "   74,\n",
       "   76,\n",
       "   85,\n",
       "   119,\n",
       "   122,\n",
       "   123,\n",
       "   147,\n",
       "   148,\n",
       "   157,\n",
       "   167,\n",
       "   184,\n",
       "   185,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   231,\n",
       "   233,\n",
       "   246,\n",
       "   247,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   268,\n",
       "   269,\n",
       "   274,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   281,\n",
       "   283,\n",
       "   287,\n",
       "   292,\n",
       "   294,\n",
       "   296,\n",
       "   306,\n",
       "   307,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   316,\n",
       "   359,\n",
       "   361,\n",
       "   362,\n",
       "   366,\n",
       "   367,\n",
       "   369,\n",
       "   370,\n",
       "   378,\n",
       "   385,\n",
       "   386,\n",
       "   437,\n",
       "   449,\n",
       "   453,\n",
       "   464,\n",
       "   468,\n",
       "   479,\n",
       "   493,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   503,\n",
       "   510,\n",
       "   512,\n",
       "   519,\n",
       "   526,\n",
       "   543,\n",
       "   544,\n",
       "   548,\n",
       "   553,\n",
       "   560,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   585],\n",
       "  [0,\n",
       "   1,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   14,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   29,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   47,\n",
       "   48,\n",
       "   51,\n",
       "   55,\n",
       "   56,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   75,\n",
       "   77,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   105,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   120,\n",
       "   121,\n",
       "   125,\n",
       "   126,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   141,\n",
       "   149,\n",
       "   152,\n",
       "   153,\n",
       "   155,\n",
       "   156,\n",
       "   158,\n",
       "   163,\n",
       "   164,\n",
       "   166,\n",
       "   169,\n",
       "   172,\n",
       "   175,\n",
       "   176,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   186,\n",
       "   194,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   214,\n",
       "   215,\n",
       "   221,\n",
       "   222,\n",
       "   225,\n",
       "   226,\n",
       "   230,\n",
       "   232,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   238,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   260,\n",
       "   264,\n",
       "   266,\n",
       "   267,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   275,\n",
       "   280,\n",
       "   282,\n",
       "   285,\n",
       "   289,\n",
       "   291,\n",
       "   299,\n",
       "   304,\n",
       "   305,\n",
       "   310,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   338,\n",
       "   339,\n",
       "   342,\n",
       "   346,\n",
       "   355,\n",
       "   357,\n",
       "   364,\n",
       "   371,\n",
       "   380,\n",
       "   382,\n",
       "   384,\n",
       "   387,\n",
       "   391,\n",
       "   394,\n",
       "   397,\n",
       "   401,\n",
       "   407,\n",
       "   412,\n",
       "   413,\n",
       "   431,\n",
       "   432,\n",
       "   434,\n",
       "   435,\n",
       "   444,\n",
       "   445,\n",
       "   452,\n",
       "   458,\n",
       "   461,\n",
       "   463,\n",
       "   465,\n",
       "   470,\n",
       "   471,\n",
       "   474,\n",
       "   481,\n",
       "   483,\n",
       "   484,\n",
       "   489,\n",
       "   491,\n",
       "   492,\n",
       "   494,\n",
       "   505,\n",
       "   507,\n",
       "   509,\n",
       "   511,\n",
       "   513,\n",
       "   516,\n",
       "   517,\n",
       "   520,\n",
       "   523,\n",
       "   524,\n",
       "   528,\n",
       "   535,\n",
       "   536,\n",
       "   538,\n",
       "   540,\n",
       "   542,\n",
       "   549,\n",
       "   550,\n",
       "   551,\n",
       "   552,\n",
       "   554,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   562,\n",
       "   566,\n",
       "   567,\n",
       "   568,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   576,\n",
       "   578,\n",
       "   579,\n",
       "   586,\n",
       "   594],\n",
       "  2,\n",
       "  -0.6200142753667681),\n",
       " ([3,\n",
       "   5,\n",
       "   6,\n",
       "   12,\n",
       "   16,\n",
       "   22,\n",
       "   27,\n",
       "   28,\n",
       "   30,\n",
       "   39,\n",
       "   40,\n",
       "   44,\n",
       "   54,\n",
       "   57,\n",
       "   58,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   73,\n",
       "   74,\n",
       "   76,\n",
       "   85,\n",
       "   119,\n",
       "   122,\n",
       "   123,\n",
       "   147,\n",
       "   148,\n",
       "   157,\n",
       "   167,\n",
       "   184,\n",
       "   185,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   231,\n",
       "   233,\n",
       "   246,\n",
       "   247,\n",
       "   256,\n",
       "   258,\n",
       "   259,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   268,\n",
       "   274,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   283,\n",
       "   287,\n",
       "   292,\n",
       "   294,\n",
       "   296,\n",
       "   306,\n",
       "   307,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   316,\n",
       "   359,\n",
       "   361,\n",
       "   362,\n",
       "   366,\n",
       "   367,\n",
       "   369,\n",
       "   370,\n",
       "   385,\n",
       "   386,\n",
       "   437,\n",
       "   453,\n",
       "   464,\n",
       "   468,\n",
       "   479,\n",
       "   493,\n",
       "   495,\n",
       "   496,\n",
       "   497,\n",
       "   498,\n",
       "   499,\n",
       "   500,\n",
       "   501,\n",
       "   503,\n",
       "   510,\n",
       "   512,\n",
       "   519,\n",
       "   526,\n",
       "   544,\n",
       "   553,\n",
       "   560,\n",
       "   580,\n",
       "   581,\n",
       "   582,\n",
       "   583,\n",
       "   585],\n",
       "  [34, 50, 66, 257, 269, 281, 378, 449, 543, 548],\n",
       "  0,\n",
       "  1.8979332013335686),\n",
       " ([1,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   14,\n",
       "   15,\n",
       "   20,\n",
       "   21,\n",
       "   23,\n",
       "   29,\n",
       "   32,\n",
       "   33,\n",
       "   36,\n",
       "   38,\n",
       "   42,\n",
       "   51,\n",
       "   55,\n",
       "   56,\n",
       "   59,\n",
       "   60,\n",
       "   62,\n",
       "   67,\n",
       "   68,\n",
       "   75,\n",
       "   78,\n",
       "   80,\n",
       "   81,\n",
       "   87,\n",
       "   90,\n",
       "   92,\n",
       "   93,\n",
       "   95,\n",
       "   99,\n",
       "   100,\n",
       "   102,\n",
       "   105,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   120,\n",
       "   121,\n",
       "   126,\n",
       "   130,\n",
       "   134,\n",
       "   141,\n",
       "   149,\n",
       "   152,\n",
       "   153,\n",
       "   155,\n",
       "   158,\n",
       "   164,\n",
       "   166,\n",
       "   169,\n",
       "   172,\n",
       "   179,\n",
       "   180,\n",
       "   182,\n",
       "   186,\n",
       "   194,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   210,\n",
       "   211,\n",
       "   215,\n",
       "   221,\n",
       "   225,\n",
       "   226,\n",
       "   230,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   238,\n",
       "   242,\n",
       "   243,\n",
       "   245,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   255,\n",
       "   264,\n",
       "   272,\n",
       "   280,\n",
       "   282,\n",
       "   285,\n",
       "   289,\n",
       "   291,\n",
       "   299,\n",
       "   310,\n",
       "   338,\n",
       "   346,\n",
       "   355,\n",
       "   364,\n",
       "   380,\n",
       "   387,\n",
       "   391,\n",
       "   394,\n",
       "   401,\n",
       "   412,\n",
       "   413,\n",
       "   431,\n",
       "   432,\n",
       "   434,\n",
       "   435,\n",
       "   444,\n",
       "   445,\n",
       "   463,\n",
       "   481,\n",
       "   492,\n",
       "   505,\n",
       "   513,\n",
       "   517,\n",
       "   520,\n",
       "   523,\n",
       "   524,\n",
       "   535,\n",
       "   536,\n",
       "   540,\n",
       "   542,\n",
       "   549,\n",
       "   551,\n",
       "   552,\n",
       "   556,\n",
       "   557,\n",
       "   558,\n",
       "   566,\n",
       "   569,\n",
       "   570,\n",
       "   571,\n",
       "   576],\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   7,\n",
       "   11,\n",
       "   17,\n",
       "   19,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   31,\n",
       "   35,\n",
       "   37,\n",
       "   41,\n",
       "   43,\n",
       "   47,\n",
       "   48,\n",
       "   61,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   77,\n",
       "   86,\n",
       "   88,\n",
       "   91,\n",
       "   94,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   101,\n",
       "   103,\n",
       "   110,\n",
       "   118,\n",
       "   125,\n",
       "   131,\n",
       "   132,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   156,\n",
       "   163,\n",
       "   175,\n",
       "   176,\n",
       "   178,\n",
       "   181,\n",
       "   203,\n",
       "   209,\n",
       "   214,\n",
       "   222,\n",
       "   232,\n",
       "   241,\n",
       "   244,\n",
       "   253,\n",
       "   254,\n",
       "   260,\n",
       "   266,\n",
       "   267,\n",
       "   270,\n",
       "   271,\n",
       "   275,\n",
       "   304,\n",
       "   305,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   339,\n",
       "   342,\n",
       "   357,\n",
       "   371,\n",
       "   382,\n",
       "   384,\n",
       "   397,\n",
       "   407,\n",
       "   452,\n",
       "   458,\n",
       "   461,\n",
       "   465,\n",
       "   470,\n",
       "   471,\n",
       "   474,\n",
       "   483,\n",
       "   484,\n",
       "   489,\n",
       "   491,\n",
       "   494,\n",
       "   507,\n",
       "   509,\n",
       "   511,\n",
       "   516,\n",
       "   528,\n",
       "   538,\n",
       "   550,\n",
       "   554,\n",
       "   562,\n",
       "   567,\n",
       "   568,\n",
       "   578,\n",
       "   579,\n",
       "   586,\n",
       "   594],\n",
       "  0,\n",
       "  0.07730391112777513),\n",
       " ([82,\n",
       "   83,\n",
       "   106,\n",
       "   140,\n",
       "   143,\n",
       "   161,\n",
       "   165,\n",
       "   173,\n",
       "   183,\n",
       "   197,\n",
       "   199,\n",
       "   200,\n",
       "   284,\n",
       "   290,\n",
       "   297,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   309,\n",
       "   314,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   331,\n",
       "   337,\n",
       "   341,\n",
       "   343,\n",
       "   345,\n",
       "   347,\n",
       "   349,\n",
       "   350,\n",
       "   351,\n",
       "   352,\n",
       "   353,\n",
       "   360,\n",
       "   368,\n",
       "   374,\n",
       "   375,\n",
       "   377,\n",
       "   381,\n",
       "   388,\n",
       "   396,\n",
       "   404,\n",
       "   406,\n",
       "   409,\n",
       "   410,\n",
       "   414,\n",
       "   416,\n",
       "   420,\n",
       "   422,\n",
       "   425,\n",
       "   429,\n",
       "   438,\n",
       "   440,\n",
       "   441,\n",
       "   451,\n",
       "   462,\n",
       "   480,\n",
       "   514,\n",
       "   515,\n",
       "   522,\n",
       "   531,\n",
       "   537,\n",
       "   565,\n",
       "   590,\n",
       "   591],\n",
       "  [13,\n",
       "   18,\n",
       "   45,\n",
       "   46,\n",
       "   49,\n",
       "   52,\n",
       "   53,\n",
       "   79,\n",
       "   84,\n",
       "   89,\n",
       "   104,\n",
       "   124,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   133,\n",
       "   142,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   150,\n",
       "   151,\n",
       "   154,\n",
       "   159,\n",
       "   160,\n",
       "   162,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   174,\n",
       "   177,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   195,\n",
       "   196,\n",
       "   198,\n",
       "   201,\n",
       "   202,\n",
       "   208,\n",
       "   212,\n",
       "   213,\n",
       "   216,\n",
       "   217,\n",
       "   223,\n",
       "   224,\n",
       "   237,\n",
       "   239,\n",
       "   240,\n",
       "   248,\n",
       "   252,\n",
       "   265,\n",
       "   273,\n",
       "   286,\n",
       "   288,\n",
       "   293,\n",
       "   295,\n",
       "   298,\n",
       "   303,\n",
       "   308,\n",
       "   315,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   340,\n",
       "   344,\n",
       "   348,\n",
       "   354,\n",
       "   356,\n",
       "   358,\n",
       "   363,\n",
       "   365,\n",
       "   372,\n",
       "   373,\n",
       "   376,\n",
       "   379,\n",
       "   383,\n",
       "   389,\n",
       "   390,\n",
       "   392,\n",
       "   393,\n",
       "   395,\n",
       "   398,\n",
       "   399,\n",
       "   400,\n",
       "   402,\n",
       "   403,\n",
       "   405,\n",
       "   408,\n",
       "   411,\n",
       "   415,\n",
       "   417,\n",
       "   418,\n",
       "   419,\n",
       "   421,\n",
       "   423,\n",
       "   424,\n",
       "   426,\n",
       "   427,\n",
       "   428,\n",
       "   430,\n",
       "   433,\n",
       "   436,\n",
       "   439,\n",
       "   442,\n",
       "   443,\n",
       "   446,\n",
       "   447,\n",
       "   448,\n",
       "   450,\n",
       "   454,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   459,\n",
       "   460,\n",
       "   466,\n",
       "   467,\n",
       "   469,\n",
       "   472,\n",
       "   473,\n",
       "   475,\n",
       "   476,\n",
       "   477,\n",
       "   478,\n",
       "   482,\n",
       "   485,\n",
       "   486,\n",
       "   487,\n",
       "   488,\n",
       "   490,\n",
       "   502,\n",
       "   504,\n",
       "   506,\n",
       "   508,\n",
       "   518,\n",
       "   521,\n",
       "   525,\n",
       "   527,\n",
       "   529,\n",
       "   530,\n",
       "   532,\n",
       "   533,\n",
       "   534,\n",
       "   539,\n",
       "   541,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   555,\n",
       "   559,\n",
       "   561,\n",
       "   563,\n",
       "   564,\n",
       "   572,\n",
       "   573,\n",
       "   574,\n",
       "   575,\n",
       "   577,\n",
       "   584,\n",
       "   587,\n",
       "   588,\n",
       "   589,\n",
       "   592,\n",
       "   593,\n",
       "   595],\n",
       "  0,\n",
       "  -1.1136340456233826),\n",
       " ([350, 351, 438, 522, 565],\n",
       "  [82,\n",
       "   83,\n",
       "   106,\n",
       "   140,\n",
       "   143,\n",
       "   161,\n",
       "   165,\n",
       "   173,\n",
       "   183,\n",
       "   197,\n",
       "   199,\n",
       "   200,\n",
       "   284,\n",
       "   290,\n",
       "   297,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   309,\n",
       "   314,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   331,\n",
       "   337,\n",
       "   341,\n",
       "   343,\n",
       "   345,\n",
       "   347,\n",
       "   349,\n",
       "   352,\n",
       "   353,\n",
       "   360,\n",
       "   368,\n",
       "   374,\n",
       "   375,\n",
       "   377,\n",
       "   381,\n",
       "   388,\n",
       "   396,\n",
       "   404,\n",
       "   406,\n",
       "   409,\n",
       "   410,\n",
       "   414,\n",
       "   416,\n",
       "   420,\n",
       "   422,\n",
       "   425,\n",
       "   429,\n",
       "   440,\n",
       "   441,\n",
       "   451,\n",
       "   462,\n",
       "   480,\n",
       "   514,\n",
       "   515,\n",
       "   531,\n",
       "   537,\n",
       "   590,\n",
       "   591],\n",
       "  0,\n",
       "  -1.9486595095523556),\n",
       " ([13,\n",
       "   18,\n",
       "   45,\n",
       "   46,\n",
       "   49,\n",
       "   79,\n",
       "   89,\n",
       "   146,\n",
       "   150,\n",
       "   151,\n",
       "   159,\n",
       "   160,\n",
       "   162,\n",
       "   177,\n",
       "   190,\n",
       "   191,\n",
       "   193,\n",
       "   213,\n",
       "   216,\n",
       "   224,\n",
       "   237,\n",
       "   239,\n",
       "   248,\n",
       "   252,\n",
       "   273,\n",
       "   293,\n",
       "   298,\n",
       "   315,\n",
       "   326,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   332,\n",
       "   333,\n",
       "   344,\n",
       "   363,\n",
       "   373,\n",
       "   383,\n",
       "   392,\n",
       "   393,\n",
       "   398,\n",
       "   400,\n",
       "   405,\n",
       "   408,\n",
       "   415,\n",
       "   417,\n",
       "   419,\n",
       "   421,\n",
       "   423,\n",
       "   426,\n",
       "   433,\n",
       "   436,\n",
       "   442,\n",
       "   443,\n",
       "   446,\n",
       "   448,\n",
       "   450,\n",
       "   454,\n",
       "   467,\n",
       "   469,\n",
       "   472,\n",
       "   477,\n",
       "   478,\n",
       "   482,\n",
       "   486,\n",
       "   487,\n",
       "   490,\n",
       "   502,\n",
       "   504,\n",
       "   506,\n",
       "   508,\n",
       "   525,\n",
       "   534,\n",
       "   539,\n",
       "   541,\n",
       "   555,\n",
       "   559,\n",
       "   564,\n",
       "   573,\n",
       "   574,\n",
       "   588,\n",
       "   589,\n",
       "   593],\n",
       "  [52,\n",
       "   53,\n",
       "   84,\n",
       "   104,\n",
       "   124,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   133,\n",
       "   142,\n",
       "   144,\n",
       "   145,\n",
       "   154,\n",
       "   168,\n",
       "   170,\n",
       "   171,\n",
       "   174,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   192,\n",
       "   195,\n",
       "   196,\n",
       "   198,\n",
       "   201,\n",
       "   202,\n",
       "   208,\n",
       "   212,\n",
       "   217,\n",
       "   223,\n",
       "   240,\n",
       "   265,\n",
       "   286,\n",
       "   288,\n",
       "   295,\n",
       "   303,\n",
       "   308,\n",
       "   324,\n",
       "   325,\n",
       "   327,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   340,\n",
       "   348,\n",
       "   354,\n",
       "   356,\n",
       "   358,\n",
       "   365,\n",
       "   372,\n",
       "   376,\n",
       "   379,\n",
       "   389,\n",
       "   390,\n",
       "   395,\n",
       "   399,\n",
       "   402,\n",
       "   403,\n",
       "   411,\n",
       "   418,\n",
       "   424,\n",
       "   427,\n",
       "   428,\n",
       "   430,\n",
       "   439,\n",
       "   447,\n",
       "   455,\n",
       "   456,\n",
       "   457,\n",
       "   459,\n",
       "   460,\n",
       "   466,\n",
       "   473,\n",
       "   475,\n",
       "   476,\n",
       "   485,\n",
       "   488,\n",
       "   518,\n",
       "   521,\n",
       "   527,\n",
       "   529,\n",
       "   530,\n",
       "   532,\n",
       "   533,\n",
       "   545,\n",
       "   546,\n",
       "   547,\n",
       "   561,\n",
       "   563,\n",
       "   572,\n",
       "   575,\n",
       "   577,\n",
       "   584,\n",
       "   587,\n",
       "   592,\n",
       "   595],\n",
       "  2,\n",
       "  -0.042288399394877496)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = []\n",
    "print(X_train)\n",
    "root_indices=list(range(0, len(X_train)))\n",
    "build_tree_recursive_continue(X_train, y_train,root_indices, \"Root\", max_depth=3, current_depth=0, tree = tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREE ENSEMBLES : RANDOM FOREST POUR LA QUALITE DU VIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prep données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides   \n",
      "0               7.4             0.700         0.00             1.9      0.076  \\\n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1138            6.3             0.510         0.13             2.3      0.076   \n",
      "1139            6.8             0.620         0.08             1.9      0.068   \n",
      "1140            6.2             0.600         0.08             2.0      0.090   \n",
      "1141            5.9             0.550         0.10             2.2      0.062   \n",
      "1142            5.9             0.645         0.12             2.0      0.075   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates   \n",
      "0                    11.0                  34.0  0.99780  3.51       0.56  \\\n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1138                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1139                 28.0                  38.0  0.99651  3.42       0.82   \n",
      "1140                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1141                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1142                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "\n",
      "      alcohol  quality    Id  \n",
      "0         9.4        5     0  \n",
      "1         9.8        5     1  \n",
      "2         9.8        5     2  \n",
      "3         9.8        6     3  \n",
      "4         9.4        5     4  \n",
      "...       ...      ...   ...  \n",
      "1138     11.0        6  1592  \n",
      "1139      9.5        6  1593  \n",
      "1140     10.5        5  1594  \n",
      "1141     11.2        6  1595  \n",
      "1142     10.2        5  1597  \n",
      "\n",
      "[1143 rows x 13 columns]\n",
      "[[ 0.93933222 -0.96338181 -0.57365783 -1.36502663]\n",
      " [ 1.94181282 -0.59360107  0.1308811  -1.36502663]\n",
      " [ 1.27349242 -0.59360107 -0.04525363 -1.16156762]\n",
      " ...\n",
      " [ 0.38239855  0.05351522 -0.45623467 -0.9581086 ]\n",
      " [ 0.10393172  0.70063152  0.60057372 -0.8563791 ]\n",
      " [ 0.6330187  -0.22382033  0.30701583 -0.75464959]]\n",
      "(559,)\n",
      "(559, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "print(df)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# load the dataset\n",
    "\n",
    "y_train = df['quality']\n",
    "X_train= [df['volatile acidity'],df['alcohol'],df['sulphates'],df['citric acid']]\n",
    "X_features = ['volatile acidity','alcohol','sulphates','citric acid']\n",
    "X_train=np.transpose(np.asmatrix(X_train))\n",
    "y_train=np.asarray(y_train)\n",
    "\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(X_norm)\n",
    "import random\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(df[\"quality\"], bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality before modif\")\n",
    "supp=[]\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 3)\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==5 or y_train[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.4):\n",
    "            supp.append(i)\n",
    "for j in range(len(supp)):\n",
    "    y_train2=np.delete(y_train,supp)\n",
    "    X_norm2=np.delete(X_norm,supp,0)\n",
    "\n",
    "\n",
    "ax[1].hist(y_train2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality after modif\")\n",
    "\n",
    "print(y_train2.shape)\n",
    "print(X_norm2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447\n",
      "[5 6 5 7 7 5 7 5 5 5 6 5 6 6 7 4 6 4 5 6 5 5 4 5 6 5 4 5 5 5 4 5 5 5 5 5 6\n",
      " 6 5 5 7 5 5 5 5 5 6 4 5 5 5 4 6 5 4 4 6 5 5 6 5 5 6 4 7 7 7 5 5 6 7 5 7 7\n",
      " 6 5 5 5 7 5 4 4 8 6 6 6 8 7 7 7 6 7 7 6 6 5 5 5 5 6 6 5 7 7 6 7 7 6 5 7 6\n",
      " 6 5 5 6 6 7 7 7 7 5 7 7 6 6 8 5 7 6 5 5 6 7 4 6 5 7 7 7 7 7 5 6 8 7 7 5 5\n",
      " 6 6 7 8 5 3 5 6 5 6 5 8 6 6 7 7 6 8 5 8 6 6 7 7 7 7 7 6 7 6 6 7 7 5 6 3 5\n",
      " 5 6 5 5 6 7 5 5 6 5 6 6 6 5 5 5 6 6 6 6 4 5 5 6 5 7 8 6 5 6 5 6 6 5 5 5 5\n",
      " 6 6 6 4 5 4 7 5 5 7 6 5 5 5 5 5 5 5 5 6 6 4 4 5 6 5 5 5 5 6 5 5 5 5 5 6 5\n",
      " 5 6 6 5 6 5 6 5 5 5 6 6 5 6 5 7 5 7 7 5 6 4 7 5 7 4 4 7 7 7 5 5 6 7 7 5 5\n",
      " 4 7 6 6 6 6 7 7 7 7 7 6 6 6 6 5 7 4 5 7 7 7 7 7 7 7 7 7 7 7 7 6 6 5 6 6 7\n",
      " 5 7 6 5 5 7 7 7 7 7 7 5 6 7 6 5 6 7 7 5 7 7 7 6 6 6 5 5 7 6 7 5 7 7 8 6 7\n",
      " 7 7 5 7 7 7 7 7 8 6 7 6 5 6 6 6 6 7 5 7 8 7 5 7 7 7 6 5 6 6 7 6 6 6 7 7 7\n",
      " 7 6 6 4 6 5 4 5 7 6 6 7 8 7 7 5 7 7 6 6 6 6 6 5 5 7 6 6 4 4 5 5 5 5 6 4 6\n",
      " 6 6 6 5 6 4 7 6 6 6 5 5 5 5 3 5 6 6 6 6 6 5 5 6 5 6 6 5 5 5 5 6 5 6 6 6 5\n",
      " 6 5 5 5 8 6 7 6 6 6 5 7 5 5 4 5 5 6 7 8 7 7 7 6 5 7 4 5 7 4 7 3 5 5 5 7 7\n",
      " 3 5 4 5 5 5 6 5 7 6 6 3 6 5 6 5 6 6 5 5 5 6 5 7 6 5 7 5 8 5 6 7 5 6 5 5 5\n",
      " 7 6 6 5]\n",
      "[2 3 2 4 4 2 4 2 2 2 3 2 3 3 4 1 3 1 2 3 2 2 1 2 3 2 1 2 2 2 1 2 2 2 2 2 3\n",
      " 3 2 2 4 2 2 2 2 2 3 1 2 2 2 1 3 2 1 1 3 2 2 3 2 2 3 1 4 4 4 2 2 3 4 2 4 4\n",
      " 3 2 2 2 4 2 1 1 5 3 3 3 5 4 4 4 3 4 4 3 3 2 2 2 2 3 3 2 4 4 3 4 4 3 2 4 3\n",
      " 3 2 2 3 3 4 4 4 4 2 4 4 3 3 5 2 4 3 2 2 3 4 1 3 2 4 4 4 4 4 2 3 5 4 4 2 2\n",
      " 3 3 4 5 2 0 2 3 2 3 2 5 3 3 4 4 3 5 2 5 3 3 4 4 4 4 4 3 4 3 3 4 4 2 3 0 2\n",
      " 2 3 2 2 3 4 2 2 3 2 3 3 3 2 2 2 3 3 3 3 1 2 2 3 2 4 5 3 2 3 2 3 3 2 2 2 2\n",
      " 3 3 3 1 2 1 4 2 2 4 3 2 2 2 2 2 2 2 2 3 3 1 1 2 3 2 2 2 2 3 2 2 2 2 2 3 2\n",
      " 2 3 3 2 3 2 3 2 2 2 3 3 2 3 2 4 2 4 4 2 3 1 4 2 4 1 1 4 4 4 2 2 3 4 4 2 2\n",
      " 1 4 3 3 3 3 4 4 4 4 4 3 3 3 3 2 4 1 2 4 4 4 4 4 4 4 4 4 4 4 4 3 3 2 3 3 4\n",
      " 2 4 3 2 2 4 4 4 4 4 4 2 3 4 3 2 3 4 4 2 4 4 4 3 3 3 2 2 4 3 4 2 4 4 5 3 4\n",
      " 4 4 2 4 4 4 4 4 5 3 4 3 2 3 3 3 3 4 2 4 5 4 2 4 4 4 3 2 3 3 4 3 3 3 4 4 4\n",
      " 4 3 3 1 3 2 1 2 4 3 3 4 5 4 4 2 4 4 3 3 3 3 3 2 2 4 3 3 1 1 2 2 2 2 3 1 3\n",
      " 3 3 3 2 3 1 4 3 3 3 2 2 2 2 0 2 3 3 3 3 3 2 2 3 2 3 3 2 2 2 2 3 2 3 3 3 2\n",
      " 3 2 2 2 5 3 4 3 3 3 2 4 2 2 1 2 2 3 4 5 4 4 4 3 2 4 1 2 4 1 4 0 2 2 2 4 4\n",
      " 0 2 1 2 2 2 3 2 4 3 3 0 3 2 3 2 3 3 2 2 2 3 2 4 3 2 4 2 5 2 3 4 2 3 2 2 2\n",
      " 4 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "X_train=X_norm2\n",
    "y_train=y_train2\n",
    "n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval\n",
    "print(int(len(y_train)*0.8)) ## Let's use 80% to train and 20% to eval\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(y_train)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.71506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-mlogloss:1.65175\n",
      "[2]\tvalidation_0-mlogloss:1.59988\n",
      "[3]\tvalidation_0-mlogloss:1.56101\n",
      "[4]\tvalidation_0-mlogloss:1.52425\n",
      "[5]\tvalidation_0-mlogloss:1.49162\n",
      "[6]\tvalidation_0-mlogloss:1.46611\n",
      "[7]\tvalidation_0-mlogloss:1.44765\n",
      "[8]\tvalidation_0-mlogloss:1.43042\n",
      "[9]\tvalidation_0-mlogloss:1.41832\n",
      "[10]\tvalidation_0-mlogloss:1.40874\n",
      "[11]\tvalidation_0-mlogloss:1.40257\n",
      "[12]\tvalidation_0-mlogloss:1.39479\n",
      "[13]\tvalidation_0-mlogloss:1.38973\n",
      "[14]\tvalidation_0-mlogloss:1.38570\n",
      "[15]\tvalidation_0-mlogloss:1.37859\n",
      "[16]\tvalidation_0-mlogloss:1.37371\n",
      "[17]\tvalidation_0-mlogloss:1.36740\n",
      "[18]\tvalidation_0-mlogloss:1.35963\n",
      "[19]\tvalidation_0-mlogloss:1.35460\n",
      "[20]\tvalidation_0-mlogloss:1.35176\n",
      "[21]\tvalidation_0-mlogloss:1.34938\n",
      "[22]\tvalidation_0-mlogloss:1.34726\n",
      "[23]\tvalidation_0-mlogloss:1.34511\n",
      "[24]\tvalidation_0-mlogloss:1.34286\n",
      "[25]\tvalidation_0-mlogloss:1.34107\n",
      "[26]\tvalidation_0-mlogloss:1.34306\n",
      "[27]\tvalidation_0-mlogloss:1.34294\n",
      "[28]\tvalidation_0-mlogloss:1.34530\n",
      "[29]\tvalidation_0-mlogloss:1.34635\n",
      "[30]\tvalidation_0-mlogloss:1.34757\n",
      "[31]\tvalidation_0-mlogloss:1.34970\n",
      "[32]\tvalidation_0-mlogloss:1.35345\n",
      "[33]\tvalidation_0-mlogloss:1.35611\n",
      "[34]\tvalidation_0-mlogloss:1.35862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8211\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8211\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
