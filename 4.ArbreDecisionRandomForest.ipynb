{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red' size=\"10\">4) Les Arbres de Décision </font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">Nous allons tester maintenant les arbres de décision. Dans ces arbres chaque noeud divise les observations en 2 branches en fonction d'une condition (par exemple alcohol<=12) et chaque feuille de l'arbre correspond à une qualité égale à la moyenne de la qualité des observations présentes dans la feuille.\n",
    "Une nouvelle observation x n'aura qu'à suivre le chemin de l'arbre en fonction des conditions des noeuds pour atterir dans une feuille. La prédiction sera alors la qualité de la feuille.</font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='blue' size=\"6\">a) Préparation des données (comme précédemment)</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1074, 10)\n",
      "(1074,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "df = pd.read_csv('WineQT.csv')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hists(df):\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "    for n in range(12):\n",
    "        i = n % 3\n",
    "        j = n % 4\n",
    "        ax[i, j].hist(df.iloc[:, n], bins='auto')\n",
    "        ax[i, j].set_xlabel(df.columns[n])\n",
    "\n",
    "#On normalise : mettre entre 0 et 1\n",
    "def normalize(df, property, parameter):\n",
    "    df[property] = np.log(df[property] + parameter)\n",
    "\n",
    "\n",
    "normalize(df, \"fixed acidity\", -2.3)\n",
    "normalize(df, \"sulphates\", -0.24)\n",
    "normalize(df, \"total sulfur dioxide\", 5)\n",
    "normalize(df, \"residual sugar\", -1.1)\n",
    "normalize(df, \"chlorides\", -0.005)\n",
    "normalize(df, \"volatile acidity\", 2)\n",
    "normalize(df, \"free sulfur dioxide\", 2)\n",
    "#plot_hists(df)\n",
    "\n",
    "standardized = (df - df.mean()) / df.std()\n",
    "standardized = standardized[(np.abs(standardized) < 3).all(axis=1)]\n",
    "rows = np.setdiff1d(list(df.index), list(standardized.index))\n",
    "df.drop(index=rows, inplace=True)\n",
    "#plot_hists(df)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Préparation des données\n",
    "y = df['quality']\n",
    "X= [df['fixed acidity'],  df['volatile acidity']  ,df['citric acid']  ,df['residual sugar'],  df['chlorides'],df['free sulfur dioxide']  ,df['total sulfur dioxide'],  df['density']    ,df['pH'],  df['sulphates']]\n",
    "X=np.transpose(np.array(X))\n",
    "y=np.asarray(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_features = ['fixed acidity',  'volatile acidity'  ,'citric acid'  ,'residual sugar',  'chlorides','free sulfur dioxide'  ,'total sulfur dioxide',  'density'    ,'pH',  'sulphates']\n",
    "nb_feature=len(X_features)\n",
    "\n",
    "\n",
    "#on supprime aleatoirement des valeurs de notes 5 et 6 (diviser par 2)\n",
    "supp=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]==5 or y[i]==6:\n",
    "        rand=random.random()\n",
    "        if(rand>0.5):\n",
    "            supp.append(i)\n",
    "y2=np.delete(y,supp)\n",
    "\n",
    "X2=np.delete(X,supp,0)\n",
    "\n",
    "#Plot des modifications\n",
    "fig,ax=plt.subplots(1,2,sharey=True)\n",
    "ax[0].hist(y, bins='auto',label=\"quality\")\n",
    "ax[0].set_title(\"quality avant modification\")\n",
    "\n",
    "ax[1].hist(y2, bins='auto',label=\"quality\")\n",
    "ax[1].set_title(\"quality après modification\")\n",
    "\n",
    "#on créé les jeux de données\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X2, y2, test_size=0.4, random_state=42)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    " # normalize the original features\n",
    "X_train, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "X_cv, X_mu, X_sigma = zscore_normalize_features(X_cv)\n",
    "X_test, X_mu, X_sigma = zscore_normalize_features(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='blue' size=\"6\">b) Mise en place d'un arbre de décision</font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">On va commencer par mettre en place un arbre de décision. Pour cela :<br>\n",
    "\n",
    "- on commence à la racine avec tout le dataset qu'on veut split.<br>\n",
    "- on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12).<br>\n",
    "- on décide le split choisi en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split.<br>\n",
    "- on sépare le dataset en fonction du meilleur split et on refait récursivement la même chose sur les 2 nouveaux noeuds.<br>\n",
    "- on s'arrête lorsque un noeud est totalement pure (=tous les vins de même qualité), ou lorsqu'on atteint une certaine profondeur.</font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='orange' size=\"5\">Codes de la structure de l'arbre pour la prédiction du vin:</font> </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbreBinaireVin:\n",
    "    def __init__(self):\n",
    "        self.qualite = 0\n",
    "        self.split=0\n",
    "        self.carac=0\n",
    "        self.enfant_gauche = None\n",
    "        self.enfant_droit = None\n",
    "\n",
    "\n",
    "    def insert_gauche(self):\n",
    "        self.enfant_gauche = ArbreBinaireVin()\n",
    "\n",
    "    def insert_droit(self):\n",
    "        self.enfant_droit = ArbreBinaireVin()\n",
    "\n",
    "    def get_valeur(self):\n",
    "        return self.valeur\n",
    "\n",
    "    def get_gauche(self):\n",
    "        return self.enfant_gauche\n",
    "\n",
    "    def get_droit(self):\n",
    "        return self.enfant_droit\n",
    "\n",
    "    \n",
    "    def get_predictionVin(self,x):\n",
    "\n",
    "        if(x[self.carac]<=self.split):\n",
    "            if(self.enfant_gauche==None):\n",
    "                return self.qualite\n",
    "            else:\n",
    "                return self.enfant_gauche.get_predictionVin(x)\n",
    "        else:\n",
    "            if(self.enfant_droit==None):\n",
    "                return self.qualite\n",
    "            else:\n",
    "                return self.enfant_droit.get_predictionVin(x)\n",
    "\n",
    "    def affiche(self):\n",
    "        print(self.carac,X_features[self.carac],self.split,self.qualite)\n",
    "        if(self.enfant_gauche!=None):\n",
    "            self.enfant_gauche.affiche()\n",
    "        if(self.enfant_droit!=None):\n",
    "            self.enfant_droit.affiche()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='orange' size=\"5\">Codes de la construction de l'arbre avec le dataset d'entrainement:</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcul de l'impureté d'un noeuf, pour savoir à quel point le noeud est pur (=les vins qui s'y trouvent ont la même qualité)\n",
    "def gini_Impurity(y):\n",
    "\n",
    "    #on calcul le nombre de valeur par note de vin (0 à 8)\n",
    "    tab_value=np.zeros(9)\n",
    "    for loop in range(len(y)):\n",
    "        tab_value[y[loop]]+=1\n",
    "    #calcul de l'impureté\n",
    "    impurity=1\n",
    "    for loop in range(len(tab_value)):\n",
    "        impurity-=(tab_value[loop]/sum(tab_value))**2\n",
    "    return impurity\n",
    "     \n",
    "#split du noeud pour des valeurs continues (ex:split en fonction de la condition {X_alcohol<=12.355?})\n",
    "def split_dataset_continue(X, node_indices, feature,t):\n",
    "\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i in node_indices:\n",
    "        if X[i,feature] <= t:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices \n",
    "\n",
    "#calcul du gain d'information = utilité d'un split, permet de choisir sur quelle condition on va split le noeud\n",
    "def compute_information_gain_continue(X, y, node_indices, feature, t):\n",
    "    \n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, feature,t)\n",
    "    \n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    information_gain = 0\n",
    "    \n",
    "    node_entropy = gini_Impurity(y_node)\n",
    "    left_entropy = gini_Impurity(y_left)\n",
    "    right_entropy = gini_Impurity(y_right)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    weighted_entropy = w_left * left_entropy + w_right * right_entropy\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "#garder la meilleur condition pour le meilleur split\n",
    "def get_best_split_continue(X, y, node_indices):   \n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    best_feature = -1\n",
    "\n",
    "    max_info_gain = 0\n",
    "    tmax=0\n",
    "\n",
    "    tab_max_feature=np.zeros(num_features)\n",
    "    tab_min_feature=np.zeros(num_features)\n",
    "    for loop in range(num_features):\n",
    "        tab_max_feature[loop]=np.max(np.transpose(X)[loop])\n",
    "        tab_min_feature[loop]=np.min(np.transpose(X)[loop])\n",
    "    \n",
    "    for feature in range(num_features):\n",
    "        tab_t_feature=np.linspace(tab_min_feature[feature], tab_max_feature[feature], len(X)-1)\n",
    "        \n",
    "        for t in range(len(tab_t_feature)):\n",
    "            info_gain = compute_information_gain_continue(X, y, node_indices, feature,tab_t_feature[t])\n",
    "\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                best_feature = feature\n",
    "                tmax=tab_t_feature[t]\n",
    "   \n",
    "    return best_feature,tmax,max_info_gain\n",
    "\n",
    "#construction recursive de l'arbre de décision:\n",
    "#on commence à la racine avec tout le dataset\n",
    "#on teste les splits sur toutes les caractéristiques du vin avec un certain nombre de valeurs (ex: sulfate <= valeurn°12)\n",
    "#on décide la condition choisie en calculant l'utilité = le gain d'information qui dépend de la pureté des noeuds résultant du split\n",
    "#on sépare le dataset en deux et on refait récursivement la même chose sur les 2 nouveaux noeuds.\n",
    "#on s'arrête lorsque un noeud est totalement pure (=tous les vins de meme qualité), ou à une certaine profondeur\n",
    "def build_tree_recursive_continue(X, y, node_indices, branch_name, max_depth, current_depth, tree, arbreVin):\n",
    "\n",
    "    if current_depth == max_depth:\n",
    "        qualite_node=np.mean(y[node_indices])\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        print(formatting,\"note moyenne attribuée à la feuille :\",qualite_node,\"(\",round(np.mean(y[node_indices])),\")\")\n",
    "\n",
    "        arbreVin.qualite=round(qualite_node)\n",
    "\n",
    "        return 0\n",
    "   \n",
    "    \n",
    "    best_feature,tmax,max_info = get_best_split_continue(X, y, node_indices) \n",
    "    arbreVin.carac=best_feature\n",
    "    arbreVin.split=tmax\n",
    "\n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %s <= %s, pour un gain de %s\" % (formatting, current_depth, branch_name, X_features[best_feature], tmax,max_info))\n",
    "\n",
    "    left_indices, right_indices = split_dataset_continue(X, node_indices, best_feature,tmax)\n",
    "    tree.append((left_indices, right_indices, best_feature,tmax))\n",
    "    \n",
    "    if(len(left_indices)>1):\n",
    "        arbreVin.insert_gauche()\n",
    "        build_tree_recursive_continue(X, y, left_indices, \"Left\", max_depth, current_depth+1, tree,arbreVin.enfant_gauche)\n",
    "    if(len(right_indices)>1):\n",
    "        arbreVin.insert_droit()\n",
    "        build_tree_recursive_continue(X, y, right_indices, \"Right\", max_depth, current_depth+1, tree,arbreVin.enfant_droit)\n",
    "    return tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">Test de l'arbre de décision avec une profondeur maximale de 4:</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42683/506145069.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  impurity-=(tab_value[loop]/sum(tab_value))**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: sulphates <= -0.09272595611982037, pour un gain de 0.052469323042744875\n",
      "- Depth 1, Left: Split on feature: total sulfur dioxide <= 0.853198588681813, pour un gain de 0.04485148373737924\n",
      "-- Depth 2, Left: Split on feature: density <= 0.41867845114657687, pour un gain de 0.04332317923273732\n",
      "--- Depth 3, Left: Split on feature: residual sugar <= 1.1300734780067812, pour un gain de 0.05027147181483804\n",
      "    ---- Left leaf node with indices [2, 6, 13, 16, 17, 27, 33, 35, 37, 39, 43, 44, 46, 49, 56, 59, 63, 64, 70, 71, 73, 74, 76, 78, 79, 88, 93, 97, 103, 105, 110, 113, 118, 119, 120, 121, 125, 130, 136, 143, 150, 163, 164, 169, 173, 177, 179, 182, 183, 187, 188, 191, 194, 200, 209, 210, 215, 216, 219, 222, 225, 226, 228, 235, 236, 241, 243, 244, 248, 252, 255, 256, 260, 264, 266, 271, 272, 281, 283, 284, 286, 290, 291, 295, 306, 311, 314, 316, 320, 332, 335, 337, 344, 345, 355, 356, 365, 369, 374]\n",
      "    ---- note moyenne attribuée à la feuille : 5.636363636363637 ( 6 )\n",
      "    ---- Right leaf node with indices [42, 81, 92, 192, 279, 310, 324, 351]\n",
      "    ---- note moyenne attribuée à la feuille : 5.875 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: total sulfur dioxide <= 0.5224497635627086, pour un gain de 0.052743342516069824\n",
      "    ---- Left leaf node with indices [11, 22, 26, 29, 30, 55, 67, 91, 109, 111, 114, 124, 148, 158, 174, 178, 186, 190, 201, 229, 233, 238, 242, 245, 246, 247, 251, 262, 267, 285, 298, 307, 318, 339, 348, 366]\n",
      "    ---- note moyenne attribuée à la feuille : 5.166666666666667 ( 5 )\n",
      "    ---- Right leaf node with indices [53, 112, 153, 185, 224, 278, 349, 353]\n",
      "    ---- note moyenne attribuée à la feuille : 5.625 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: fixed acidity <= -2.8656468717590577, pour un gain de 0.036946019743751335\n",
      "--- Depth 3, Right: Split on feature: free sulfur dioxide <= 0.6597548370368371, pour un gain de 0.03752486629679613\n",
      "    ---- Left leaf node with indices [4, 5, 9, 57, 68, 80, 87, 123, 128, 217, 232, 240, 249, 265, 277, 303, 338, 357, 367]\n",
      "    ---- note moyenne attribuée à la feuille : 5.315789473684211 ( 5 )\n",
      "    ---- Right leaf node with indices [12, 47, 62, 65, 69, 104, 106, 116, 134, 137, 140, 146, 152, 154, 168, 176, 193, 206, 214, 254, 287, 299, 301, 333, 336, 362]\n",
      "    ---- note moyenne attribuée à la feuille : 5.038461538461538 ( 5 )\n",
      "- Depth 1, Right: Split on feature: citric acid <= 0.15676519921971788, pour un gain de 0.061485402736593864\n",
      "-- Depth 2, Left: Split on feature: pH <= 0.3387519339878331, pour un gain de 0.05154936838208535\n",
      "--- Depth 3, Left: Split on feature: density <= 0.6547929676033211, pour un gain de 0.1570550931430834\n",
      "    ---- Left leaf node with indices [0, 19, 32, 51, 52, 58, 61, 66, 115, 122, 138, 159, 160, 184, 258, 261, 276, 315, 317, 325, 327, 329, 330, 375]\n",
      "    ---- note moyenne attribuée à la feuille : 5.625 ( 6 )\n",
      "    ---- Right leaf node with indices [25, 86, 94, 142, 151]\n",
      "    ---- note moyenne attribuée à la feuille : 6.0 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: fixed acidity <= -2.350583798240976, pour un gain de 0.1004821389436773\n",
      "    ---- Left leaf node with indices [135, 141, 308]\n",
      "    ---- note moyenne attribuée à la feuille : 7.0 ( 7 )\n",
      "    ---- Right leaf node with indices [10, 20, 21, 24, 40, 45, 101, 102, 107, 132, 133, 144, 157, 165, 172, 203, 208, 213, 218, 220, 223, 227, 250, 274, 288, 289, 300, 309, 321, 334, 340, 347, 350, 364, 372, 373]\n",
      "    ---- note moyenne attribuée à la feuille : 5.75 ( 6 )\n",
      "-- Depth 2, Right: Split on feature: total sulfur dioxide <= 0.7429489803087783, pour un gain de 0.027252765090602815\n",
      "--- Depth 3, Left: Split on feature: density <= 0.4029374833827939, pour un gain de 0.03418658363713323\n",
      "    ---- Left leaf node with indices [1, 3, 7, 8, 15, 18, 23, 28, 34, 38, 48, 50, 54, 60, 75, 77, 83, 85, 89, 98, 99, 100, 117, 126, 129, 147, 155, 161, 162, 166, 180, 199, 202, 204, 207, 230, 231, 253, 263, 268, 270, 280, 296, 304, 322, 323, 326, 328, 341, 342, 343, 346, 370, 371]\n",
      "    ---- note moyenne attribuée à la feuille : 6.888888888888889 ( 7 )\n",
      "    ---- Right leaf node with indices [14, 36, 41, 72, 96, 127, 145, 149, 156, 171, 175, 181, 196, 197, 198, 211, 212, 221, 234, 237, 257, 259, 269, 275, 293, 294, 302, 312, 313, 319, 352, 354, 359, 360, 361, 363, 368]\n",
      "    ---- note moyenne attribuée à la feuille : 6.486486486486487 ( 6 )\n",
      "--- Depth 3, Right: Split on feature: total sulfur dioxide <= 1.245197196230381, pour un gain de 0.13250000000000017\n",
      "    ---- Left leaf node with indices [82, 84, 90, 95, 139, 167, 189, 195, 205, 282, 305, 331]\n",
      "    ---- note moyenne attribuée à la feuille : 5.916666666666667 ( 6 )\n",
      "    ---- Right leaf node with indices [31, 108, 131, 170, 239, 292, 297, 358]\n",
      "    ---- note moyenne attribuée à la feuille : 6.25 ( 6 )\n"
     ]
    }
   ],
   "source": [
    "tree = []\n",
    "arbre = ArbreBinaireVin()\n",
    "root_indices=list(range(0, len(X_train)))\n",
    "build_tree_recursive_continue(X_train, y_train,root_indices, \"Root\", max_depth=4, current_depth=0, tree = tree, arbreVin=arbre);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vecteur x [ 1.65989152 -0.56212655  1.29038469 -1.73235125  0.93061168  0.88161709\n",
      "  0.30550152  0.39591505 -1.94880838  0.87331486]\n",
      "note prédite: 7\n",
      "vraie note : 7\n"
     ]
    }
   ],
   "source": [
    "x=X_train[50]\n",
    "print(\"vecteur x\",x)\n",
    "print(\"note prédite:\",arbre.get_predictionVin(x))\n",
    "print(\"vraie note :\",y_train[50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">On a construit l'arbre de profondeur max 4, il ne reste plus qu'à faire passer notre jeu de test dedans.</font> </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_error(yhat,y_test):\n",
    "   \n",
    "    cost = 0.0\n",
    "    m=len(y_test)\n",
    "    for i in range(m):                                \n",
    "        cost = cost + (y_test[i] - yhat[i])**2   #scalar\n",
    "    cost = cost / (2 * m)                 #scalar    \n",
    "    return cost\n",
    "\n",
    "def predict_model(arbre,X,y):\n",
    "    yhat=[]\n",
    "    for loop in range(len(X)):\n",
    "        yhat.append(arbre.get_predictionVin(X[loop]))\n",
    "\n",
    "    sum=0\n",
    "    for loop in range(len(yhat)):\n",
    "        if(y[loop] == yhat[loop]):\n",
    "            sum+=1\n",
    "    print('Train Accuracy (%): ',(sum/len(yhat))*100)\n",
    "\n",
    "    nn_test_error =calcul_error(yhat,y)\n",
    "\n",
    "    print(f\"Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (%):  53.96825396825397\n",
      "Set Classification Error: 0.3135\n"
     ]
    }
   ],
   "source": [
    "predict_model(arbre,X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">Nous obtenons 53% de prédictions exactes sur le jeu de test.\n",
    "Pour améliorer ce résultat, nous allons utiliser une version évoluée de l'arbre de décision : la Random Forest.\n",
    "</font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='blue' size=\"6\">c) Forêt d'arbres décisionnels pour la qualité du vin </font> </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">Nous allons enfin mettre en place une random forest ou forêt d'arbres décisionnels à l'aide la bibliothèque sklearn et xgboost.\n",
    "Random forest va créer une multitude d'arbres de décision de ce type avec une part d'aléatoire dans le choix des splits et choisir l'arbre final grâce à un système de votes.</font> </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "\n",
    "RANDOM_STATE = 55 ## We will pass it to every sklearn call so we ensure reproducibility\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test=le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.76413\n",
      "[1]\tvalidation_0-mlogloss:1.74212\n",
      "[2]\tvalidation_0-mlogloss:1.74340\n",
      "[3]\tvalidation_0-mlogloss:1.72256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri/.local/lib/python3.8/site-packages/xgboost/sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-mlogloss:1.72616\n",
      "[5]\tvalidation_0-mlogloss:1.69494\n",
      "[6]\tvalidation_0-mlogloss:1.69315\n",
      "[7]\tvalidation_0-mlogloss:1.69298\n",
      "[8]\tvalidation_0-mlogloss:1.70397\n",
      "[9]\tvalidation_0-mlogloss:1.72146\n",
      "[10]\tvalidation_0-mlogloss:1.72980\n",
      "[11]\tvalidation_0-mlogloss:1.74967\n",
      "[12]\tvalidation_0-mlogloss:1.76279\n",
      "[13]\tvalidation_0-mlogloss:1.77983\n",
      "[14]\tvalidation_0-mlogloss:1.78464\n",
      "[15]\tvalidation_0-mlogloss:1.79472\n",
      "[16]\tvalidation_0-mlogloss:1.79150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train,y_train, eval_set = [(X_test,y_test)], early_stopping_rounds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics train:\n",
      "\tAccuracy score: 0.8093\n",
      "Metrics test:\n",
      "\tAccuracy score: 0.8093\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\")\n",
    "\n",
    "print(xgb_model.classes_)\n",
    "#print(xgb_model.classes_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='black' size=\"5\">Le modèle random Forest nous permet d'obtenir sur le jeu de test 80% de précision, c'est le meilleur score obtenu. On choisit donc cette méthode</font> </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
